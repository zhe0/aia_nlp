{"cells":[{"cell_type":"markdown","id":"a2ef7047","metadata":{"id":"a2ef7047"},"source":["# Aggregation function in Graph Neural Network\n","\n","Generalizing the convolution operator to irregular domains is typically expressed as a neighborhood aggregation or message passing scheme. With $\\mathbf{x}_i^{(k-1)} \\in \\mathbb{R}^F$\n"," denoting node features of node $i$ in $(k-1)$ layer  and $\\mathbf{e}_{j,i} \\in \\mathbb{R}^D$  denoting (optional) edge features from node  to node , message passing graph neural networks can be described as\n","\n","$$\n","\\mathbf{x}_i^{(k)} = \\gamma^{(k)} \\left( \\mathbf{x}_i^{(k-1)}, \\square_{j \\in \\mathcal{N}(i)} \\, \\phi^{(k)}\\left(\\mathbf{x}_i^{(k-1)}, \\mathbf{x}_j^{(k-1)},\\mathbf{e}_{j,i}\\right) \\right),\n","$$\n","\n","where \n","* $\\square$ denotes a differentiable, permutation invariant function, e.g., sum, mean or max\n","* $\\gamma$  and  $\\phi$ denote differentiable functions such as MLPs (Multi Layer Perceptrons).\n","\n","![](https://i.imgur.com/Q291Xuq.png)\n","\n","\n"]},{"cell_type":"markdown","id":"fe5861cb","metadata":{"id":"fe5861cb"},"source":["The \"MessagePassing\" Base Class\n","-------------------------------\n","\n","PyG provides the `MessagePassing` base class, which helps in creating such kinds of message passing graph neural networks by automatically taking care of message propagation.\n","The user only has to define the functions $\\phi$ , *i.e.* `MessagePassing.message`, and $\\gamma$ , *i.e.* `MessagePassing.update`, as well as the aggregation scheme to use, *i.e.* `aggr=\"add\"`, `aggr=\"mean\"` or `aggr=\"max\"`.\n","\n","This is done with the help of the following methods:\n","\n","* `MessagePassing(aggr=\"add\")`: Defines the aggregation scheme to use (`\"add\"`, `\"mean\"` or `\"max\"`).\n","* `MessagePassing.propagate(edge_index, size=None, **kwargs)`:\n","  The initial call to start propagating messages.\n","  Takes in the edge indices and all additional data which is needed to construct messages and to update node embeddings.\n","* `MessagePassing.message(...)`: Constructs messages to node `i` in analogy to $\\phi$ for each edge $(j,i) \\in \\mathcal{E}$.Can take any argument which was initially passed to `propagate`.\n","  In addition, tensors passed to `propagate` can be mapped to the respective nodes `i` and `j` by appending `_i` or `_j` to the variable name, *e.g.* `x_i` and `x_j`.\n","  Note that we generally refer to `i` as the central nodes that aggregates information, and refer to `j` as the neighboring nodes, since this is the most common notation."]},{"cell_type":"markdown","id":"1438448e","metadata":{"id":"1438448e"},"source":["## Implement a very simple GNN module\n","Let's first implement a simple GNN layer!<br>\n","The **message** of node $i$ and node $j$ is defined by the concatenation of their embedding and transformed by a weight matrix.  <br>\n","Finally, the embedding $x_i$ of node $i$ is the summation of the message of its neighbor $N(i)$.<br>\n","\n","\n","$$\n","\\mathbf{x}_i^{(k)} = \\sum_{j \\in \\mathcal{N}(i)} \\left( \\mathbf{W}^{\\top} \\cdot \\left( \\mathbf{x}_i^{(k-1)} || \\mathbf{x}_j^{(k-1)} \\right)  \\right),\n","$$"]},{"cell_type":"code","source":["import os\n","import torch\n","import numpy as np\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install -q torch-cluster -f https://data.pyg.org/whl/torch-${TORCH}.html\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yneDIcLLA4O1","executionInfo":{"status":"ok","timestamp":1658763619703,"user_tz":-480,"elapsed":25478,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}},"outputId":"8f34e0c6-e2fa-40ae-b609-f22eb88f81da"},"id":"yneDIcLLA4O1","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["1.12.0+cu113\n","\u001b[K     |████████████████████████████████| 7.9 MB 38.0 MB/s \n","\u001b[K     |████████████████████████████████| 3.5 MB 26.7 MB/s \n","\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 2.4 MB 17.0 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","execution_count":2,"id":"8fc2aa80","metadata":{"id":"8fc2aa80","executionInfo":{"status":"ok","timestamp":1658763620694,"user_tz":-480,"elapsed":1006,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.nn import Linear, Parameter\n","from torch_geometric.nn import MessagePassing\n","\n","class BasicGNN(MessagePassing):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__(aggr='add')  # Sum aggregation\n","        self.lin = Linear(in_channels*2, out_channels, bias=False)\n","        self.reset_parameters()\n","    \n","    def reset_parameters(self):\n","        for p in self.parameters():\n","            if len(p.shape) > 1:\n","                torch.nn.init.ones_(p)\n","            else:\n","                torch.nn.init.zeros_(p)\n","\n","    def forward(self, x, edge_index):\n","        # x has shape [N, in_channels]\n","        # edge_index has shape [2, E]\n","\n","        # Obtain the message of each edge\n","        # messages has shape [E, in_channels*2]\n","        messages = self.propagate(edge_index, x=x)\n","        \n","        # Apply linear transformation\n","        out = self.lin(messages)\n","\n","        return out\n","\n","    def message(self, x_i, x_j):\n","        \n","        # x_i, x_j has shape [E, in_channels]\n","        return torch.cat([x_i,x_j],dim=-1)"]},{"cell_type":"code","execution_count":3,"id":"9886bce7","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9886bce7","executionInfo":{"status":"ok","timestamp":1658763620694,"user_tz":-480,"elapsed":10,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}},"outputId":"fac10ea7-5ee2-4fa0-847c-887b233e5c63"},"outputs":[{"output_type":"stream","name":"stdout","text":["Node feature:\n"," tensor([[0., 1., 2.],\n","        [3., 4., 5.],\n","        [6., 7., 8.]])\n","Edges:\n"," tensor([[0, 1],\n","        [1, 2]])\n"]}],"source":["node_feature = torch.arange(9).view(3,3).float()\n","edge_index = torch.LongTensor([\n","    [0,1],\n","    [1,2]\n","])\n","print(\"Node feature:\\n\",node_feature)\n","print(\"Edges:\\n\",edge_index)"]},{"cell_type":"code","execution_count":4,"id":"c70954c1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c70954c1","executionInfo":{"status":"ok","timestamp":1658763620695,"user_tz":-480,"elapsed":9,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}},"outputId":"c2397566-808b-4a7f-e2ab-9a017a721604"},"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n"," tensor([ 0., 15., 33.])\n"]}],"source":["GCN_model = BasicGNN(3,1)\n","gcn_output = GCN_model(node_feature, edge_index).flatten().detach()\n","print(\"Output:\\n\",gcn_output)"]},{"cell_type":"markdown","id":"79fe1888","metadata":{"id":"79fe1888"},"source":["## Implementing the GCN Layer\n","The GCN layer is mathematically defined as\n","$$\n","\\mathbf{x}_i^{(k)} = \\sum_{j \\in \\mathcal{N}(i) \\cup \\{ i \\}} \\frac{1}{\\sqrt{\\deg(i)} \\cdot \\sqrt{\\deg(j)}} \\cdot \\left( \\mathbf{W}^{\\top} \\cdot \\mathbf{x}_j^{(k-1)} \\right) + \\mathbf{b},\n","$$\n","where neighboring node features are first transformed by a weight matrix $\\mathbf{W}$, normalized by their degree, and finally summed up.\n","Lastly, we apply the bias vector $\\mathbf{b}$ to the aggregated output.\n","This formula can be divided into the following steps:\n","\n","1. Add self-loops to the adjacency matrix.\n","2. Linearly transform node feature matrix.\n","3. Compute normalization coefficients.\n","4. Normalize node features in $`\\phi$.\n","5. Sum up neighboring node features (`\"add\"` aggregation).\n","6. Apply a final bias vector.\n","\n","Steps 1-3 are typically computed before message passing takes place.\n","Steps 4-5 can be easily processed using the `MessagePassing` base class.\n","The full layer implementation is shown below:\n"]},{"cell_type":"code","execution_count":5,"id":"bd111ca4","metadata":{"id":"bd111ca4","executionInfo":{"status":"ok","timestamp":1658763620695,"user_tz":-480,"elapsed":8,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}}},"outputs":[],"source":["from torch_geometric.utils import add_self_loops, degree\n","\n","class GCNConv(MessagePassing):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n","        self.lin = Linear(in_channels, out_channels, bias=False)\n","        self.bias = Parameter(torch.Tensor(out_channels))\n","        self.reset_parameters()\n","    \n","    def reset_parameters(self):\n","        for p in self.parameters():\n","            if len(p.shape) > 1:\n","                torch.nn.init.ones_(p)\n","            else:\n","                torch.nn.init.zeros_(p)\n","\n","    def forward(self, x, edge_index):\n","        # x has shape [N, in_channels]\n","        # edge_index has shape [2, E]\n","\n","        # Step 1: Add self-loops to the adjacency matrix.\n","        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n","\n","        # Step 2: Linearly transform node feature matrix.\n","        x = self.lin(x)\n","\n","        # Step 3: Compute normalization.\n","        row, col = edge_index\n","        deg = degree(col, x.size(0), dtype=x.dtype)\n","        deg_inv_sqrt = deg.pow(-0.5)\n","        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n","        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n","\n","        # Step 4-5: Start propagating messages.\n","        out = self.propagate(edge_index, x=x, norm=norm)\n","\n","        # Step 6: Apply a final bias vector.\n","        out += self.bias\n","\n","        return out\n","\n","    def message(self, x_j, norm):\n","        # x_j has shape [E, out_channels]\n","\n","        # Step 4: Normalize node features.\n","        return norm.view(-1, 1) * x_j"]},{"cell_type":"code","execution_count":6,"id":"e23200b9","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e23200b9","executionInfo":{"status":"ok","timestamp":1658763620695,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}},"outputId":"ecce41b3-d00b-4986-b5d1-0a280b2abb31"},"outputs":[{"output_type":"stream","name":"stdout","text":["Node feature:\n"," tensor([[0., 1., 2.],\n","        [3., 4., 5.],\n","        [6., 7., 8.]])\n","Edges:\n"," tensor([[0],\n","        [1]])\n"]}],"source":["node_feature = torch.arange(9).view(3,3).float()\n","edge_index = torch.LongTensor([\n","    [0,],\n","    [1,]\n","])\n","print(\"Node feature:\\n\",node_feature)\n","print(\"Edges:\\n\",edge_index)"]},{"cell_type":"code","execution_count":7,"id":"6ab88929","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6ab88929","executionInfo":{"status":"ok","timestamp":1658763620695,"user_tz":-480,"elapsed":6,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}},"outputId":"bf84c4ff-6a4c-4f86-98a3-d96a454d6c7f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n"," tensor([ 3.0000,  8.1213, 21.0000])\n","Expected:\n"," tensor([ 3.0000,  8.1213, 21.0000])\n"]}],"source":["GCN_model = GCNConv(3,1)\n","gcn_output = GCN_model(node_feature, edge_index).flatten().detach()\n","print(\"Output:\\n\",gcn_output)\n","\n","self_degree = torch.FloatTensor([1,2,1])\n","self_feature = node_feature.sum(dim=-1) / self_degree\n","neighbor_feature = torch.FloatTensor([\n","    0,\n","    (0+1+2) / (1**0.5 * 2**0.5),\n","    0,\n","])\n","expected_solution = self_feature + neighbor_feature\n","print(\"Expected:\\n\",expected_solution)"]},{"cell_type":"markdown","id":"3cfa454b","metadata":{"id":"3cfa454b"},"source":["## Practice: GraphSage Implementation\n","\n","Now let's start working on our own implementation of layers! This part is to get you familiar with how to implement Pytorch layer based on Message Passing. You will be implementing the **forward**, **message** and **aggregate** functions.\n","\n","Generally, the **forward** function is where the actual message passing is conducted. All logic in each iteration happens in **forward**, where we'll call **propagate** function to propagate information from neighbor nodes to central nodes.  So the general paradigm will be pre-processing -> propagate -> post-processing.\n","\n","Recall the process of message passing we introduced in homework 1. **propagate** further calls **message** which transforms information of neighbor nodes into messages, **aggregate** which aggregates all messages from neighbor nodes into one, and **update** which further generates the embedding for nodes in the next iteration.\n","\n","Our implementation is slightly variant from this, where we'll not explicitly implement **update**, but put the logic for updating nodes in **forward** function. To be more specific, after information is propagated, we can further conduct some operations on the output of **propagate**. The output of **forward** is exactly the embeddings after the current iteration.\n","\n","In addition, tensors passed to **propagate()** can be mapped to the respective nodes $i$ and $j$ by appending _i or _j to the variable name, .e.g. x_i and x_j. Note that we generally refer to $i$ as the central nodes that aggregates information, and refer to $j$ as the neighboring nodes, since this is the most common notation.\n","\n","Please find more details in the comments. One thing to note is that we're adding **skip connections** to our GraphSage. Formally, the update rule for our model is described as below:\n","\n","\\begin{equation}\n","h_v^{(l)} = W_l\\cdot h_v^{(l-1)} + W_r \\cdot AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\})\n","\\end{equation}\n","\n","For simplicity, we use mean aggregations where:\n","\n","\\begin{equation}\n","AGG(\\{h_u^{(l-1)}, \\forall u \\in N(v) \\}) = \\frac{1}{|N(v)|} \\sum_{u\\in N(v)} h_u^{(l-1)}\n","\\end{equation}\n","\n","Additionally, $\\ell$-2 normalization is applied after each iteration.\n","\n","\\begin{equation}\n","h_v^{(l)} = h_v^{(l)} /  \\lVert h_v^{(l)}\\rVert_2, \\forall v \\in \\mathcal{V}\n","\\end{equation}\n","\n"]},{"cell_type":"code","execution_count":8,"id":"bf8c9080","metadata":{"id":"bf8c9080","executionInfo":{"status":"ok","timestamp":1658763620695,"user_tz":-480,"elapsed":5,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}}},"outputs":[],"source":["class GraphSage(MessagePassing):\n","    \n","    def __init__(self, in_channels, out_channels, normalize = True,\n","                 bias = False, heads=1, **kwargs):  \n","        super(GraphSage, self).__init__(**kwargs)\n","\n","        self.in_channels = in_channels\n","        self.out_channels = out_channels\n","        self.normalize = normalize\n","\n","        self.lin_l = None\n","        self.lin_r = None\n","\n","        ############################################################################\n","        # TODO: Your code here! \n","        # Define the layers needed for the message and update functions below.\n","        # self.lin_l is the linear transformation that you apply to embedding \n","        #            for central node.\n","        # self.lin_r is the linear transformation that you apply to aggregated \n","        #            message from neighbors.\n","        # Our implementation is ~2 lines, but don't worry if you deviate from this.\n","        self.lin_l = nn.Linear(self.in_channels, self.out_channels,bias=False)\n","        self.lin_r = nn.Linear(self.in_channels, self.out_channels,bias=False)\n","\n","        ############################################################################\n","\n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for p in self.parameters():\n","            if len(p.shape) > 1:\n","                torch.nn.init.ones_(p)\n","            else:\n","                torch.nn.init.zeros_(p)\n","\n","    def forward(self, x, edge_index, size = None):\n","        \"\"\"\"\"\"\n","\n","        out = None\n","\n","        ############################################################################\n","        # TODO: Your code here! \n","        # Implement message passing, as well as any post-processing (our update rule).\n","        # 1. First call propagate function to conduct the message passing.\n","        #    1.1 See there for more information: \n","        #        https://pytorch-geometric.readthedocs.io/en/latest/notes/create_gnn.html\n","        #    1.2 We use the same representations for central (x_central) and \n","        #        neighbor (x_neighbor) nodes, which means you'll pass x=(x, x) \n","        #        to propagate.\n","        # 2. Update our node embedding with skip connection.\n","        # 3. If normalize is set, do L-2 normalization (defined in \n","        #    torch.nn.functional)\n","        # Our implementation is ~5 lines, but don't worry if you deviate from this.\n","        x_prop = self.propagate(edge_index, x=x, size=size)\n","        x = self.lin_l(x) + x_prop\n","        \n","        if self.normalize:\n","            x = F.normalize(x)\n","        \n","        out = x\n","        \n","        ############################################################################\n","\n","        return out\n","\n","    def message(self, x_j):\n","\n","        out = None\n","\n","        ############################################################################\n","        # TODO: Your code here! \n","        # Implement your message function here.\n","        # Our implementation is ~1 lines, but don't worry if you deviate from this.\n","        out = self.lin_r(x_j)\n","\n","        ############################################################################\n","\n","        return out"]},{"cell_type":"code","execution_count":9,"id":"010736c8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"010736c8","executionInfo":{"status":"ok","timestamp":1658763621200,"user_tz":-480,"elapsed":510,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}},"outputId":"04986ffb-3d40-46ab-9ac5-fcfe27ac5b3d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Node feature:\n"," tensor([[0., 1., 2.],\n","        [3., 4., 5.],\n","        [6., 7., 8.]])\n","Edges:\n"," tensor([[0, 1, 2],\n","        [1, 2, 1]])\n"]}],"source":["node_feature = torch.arange(9).view(3,3).float()\n","edge_index = torch.LongTensor([\n","    [0,1,2],\n","    [1,2,1]\n","])\n","print(\"Node feature:\\n\",node_feature)\n","print(\"Edges:\\n\",edge_index)"]},{"cell_type":"markdown","id":"195473a8","metadata":{"id":"195473a8"},"source":["## Check you answer!\n","Generate the anwers(output) from `SAGE_model` and calculate the expected output manually. <br>\n","In addition, use the official implementation `torch_geometric.nn.SAGEConv` and see if the output match! \n"]},{"cell_type":"code","execution_count":10,"id":"f5a7aa66","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f5a7aa66","executionInfo":{"status":"ok","timestamp":1658763621201,"user_tz":-480,"elapsed":7,"user":{"displayName":"Yu-Che Tsai","userId":"11097797349614937936"}},"outputId":"653bd007-929b-4ea6-b62d-04b064e91497"},"outputs":[{"output_type":"stream","name":"stdout","text":["Output:\n"," tensor([ 3., 24., 33.])\n","Expected:\n"," tensor([ 3., 24., 33.])\n","Official Output:\n"," tensor([ 3., 24., 33.])\n"]}],"source":["SAGE_model = GraphSage(3,1,normalize=False, aggr=\"mean\")\n","sage_output = SAGE_model(node_feature, edge_index).flatten().detach()\n","print(\"Output:\\n\",sage_output)\n","\n","expected_solution = None\n","################################\n","# TODO: Your code here! \n","# Caluate the answer manually!\n","self_feature = node_feature.sum(dim=-1)\n","neighbor_feature = torch.FloatTensor([\n","    0,\n","    torch.mean(self_feature[[0,2]]),\n","    torch.mean(self_feature[[1]])\n","])\n","expected_solution = self_feature + neighbor_feature\n","################################\n","print(\"Expected:\\n\",expected_solution)\n","\n","official_sage_output = None\n","################################\n","# TODO: Your code here! \n","# 1. import SAGEConv from torch_geometric.nn \n","# 2. generate the output\n","# 3. Remember to initialize the weight matrix with ones, and biases with zeros\n","\n","from torch_geometric.nn import SAGEConv\n","pyg_SAGE = SAGEConv(3,1,normalize=False,aggr=\"mean\")\n","for p in pyg_SAGE.parameters():\n","    if len(p.shape) > 1:\n","        torch.nn.init.ones_(p)\n","    else:\n","        torch.nn.init.zeros_(p)\n","official_sage_output = pyg_SAGE(node_feature, edge_index).flatten().detach()\n","################################\n","print(\"Official Output:\\n\",official_sage_output)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"3-GNN_Aggregation_Function_Solution.ipynb","provenance":[]},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}