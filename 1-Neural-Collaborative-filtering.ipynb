{"cells":[{"cell_type":"markdown","id":"0415bf40","metadata":{"id":"0415bf40"},"source":["# Neural Collaborative filtering\n","Previosuly, we introduce the basics of collaborative filtering using item and user-based CF. <br>\n","However, traditional CF cannot capture more general pattern due to the simplicity of its method.<br>\n","Therefore, people resort to neural networks to learn more complex, fine-grained inforamtion from user-item interaction. <br>\n","In this lecture, we will implement two classic NN-based CF model including: **BPR-MF** and **NeuMF**."]},{"cell_type":"markdown","id":"a3e6fa5a","metadata":{"id":"a3e6fa5a"},"source":["## Data preparation\n","Here we will use the most common, famous dataset: Movielens which contains the user-movie interactions."]},{"cell_type":"code","execution_count":null,"id":"4748fb60","metadata":{"id":"4748fb60"},"outputs":[],"source":["# Install required packages.\n","import os\n","import torch\n","os.environ['TORCH'] = torch.__version__\n","print(torch.__version__)\n","\n","!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html\n","!pip install -q git+https://github.com/pyg-team/pytorch_geometric.git\n","!pip install -q git+https://github.com/snap-stanford/deepsnap.git\n","!pip install -U -q PyDrive"]},{"cell_type":"code","execution_count":null,"id":"b32ad5aa","metadata":{"id":"b32ad5aa"},"outputs":[],"source":["# import required modules\n","import random\n","from tqdm import tqdm\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","from torch import nn, optim, Tensor\n","import torch.nn.functional as F\n","from torch_geometric.data import download_url, extract_zip\n","from torch_geometric.utils import structured_negative_sampling"]},{"cell_type":"code","execution_count":null,"id":"b44359c2","metadata":{"id":"b44359c2"},"outputs":[],"source":["# download the dataset\n","url = 'https://files.grouplens.org/datasets/movielens/ml-latest-small.zip'\n","extract_zip(download_url(url, '.'), '.')\n","movie_path = './ml-latest-small/movies.csv'\n","rating_path = './ml-latest-small/ratings.csv'"]},{"cell_type":"code","execution_count":null,"id":"db1244d9","metadata":{"id":"db1244d9"},"outputs":[],"source":["# load user and movie nodes\n","def load_mapping(path, index_col):\n","    \"\"\"Loads csv containing interaction information\n","\n","    Args:\n","        path (str): path to csv file\n","        index_col (str): column name of index column\n","\n","    Returns:\n","        dict: mapping of csv row to unique id\n","    \"\"\"\n","    df = pd.read_csv(path, index_col=index_col)\n","    \n","    # assign unique index for each user/movie\n","    mapping = {index: i for i, index in enumerate(df.index.unique())}\n","    return mapping\n","\n","\n","user_mapping = load_mapping(rating_path, index_col='userId')\n","movie_mapping = load_mapping(movie_path, index_col='movieId')"]},{"cell_type":"code","execution_count":null,"id":"fe36f0d5","metadata":{"id":"fe36f0d5"},"outputs":[],"source":["# load edges between users and movies\n","def load_interaction(path, src_index_col, src_mapping, dst_index_col, dst_mapping, link_index_col, rating_threshold=4):\n","    \"\"\"Loads csv containing edges between users and items\n","\n","    Args:\n","        path (str): path to csv file\n","        src_index_col (str): column name of users\n","        src_mapping (dict): mapping between row number and user id\n","        dst_index_col (str): column name of items\n","        dst_mapping (dict): mapping between row number and item id\n","        link_index_col (str): column name of user item interaction\n","        rating_threshold (int, optional): Threshold to determine positivity of edge. Defaults to 4.\n","\n","    Returns:\n","        torch.Tensor: N by 2 matrix containing the user item interaction\n","    \"\"\"\n","    df = pd.read_csv(path)\n","    edge_index = None\n","    src = [src_mapping[index] for index in df[src_index_col]]\n","    dst = [dst_mapping[index] for index in df[dst_index_col]]\n","    is_positive = torch.from_numpy(df[link_index_col].values).view(-1, 1).to(torch.long) >= rating_threshold\n","\n","    interactions = []\n","    for i in range(is_positive.shape[0]):\n","        if is_positive[i]:\n","            interactions.append([src[i],dst[i]])\n","\n","    return torch.tensor(interactions)\n","\n","\n","interactions = load_interaction(\n","    rating_path,\n","    src_index_col='userId',\n","    src_mapping=user_mapping,\n","    dst_index_col='movieId',\n","    dst_mapping=movie_mapping,\n","    link_index_col='rating',\n","    rating_threshold=4,\n",")"]},{"cell_type":"code","execution_count":null,"id":"3f11250d","metadata":{"id":"3f11250d"},"outputs":[],"source":["# split the edges of the graph using a 80/10/10 train/validation/test split\n","num_users, num_movies = len(user_mapping), len(movie_mapping)\n","num_interactions = interactions.shape[0]\n","all_indices = [i for i in range(num_interactions)]\n","\n","train_indices, test_indices = train_test_split(\n","    all_indices, test_size=0.2, random_state=1)\n","val_indices, test_indices = train_test_split(\n","    test_indices, test_size=0.5, random_state=1)\n","\n","train_interactions = interactions[train_indices,:]\n","val_interactions = interactions[val_indices,:]\n","test_interactions = interactions[test_indices,:]"]},{"cell_type":"markdown","id":"03c0ea70","metadata":{"id":"03c0ea70"},"source":["## The BPR-MF model\n","BPR-MF is a matrix factorization model that factorize the user-item interaction matrix with user and item embedding matrix.\n","![](https://i.imgur.com/FM1w89a.png)\n","\n","## Loss Function\n","\n","\n","\n","We utilize a Bayesian Personalized Ranking (BPR) loss, a pairwise objective which encourages the predictions of positive samples to be higher than negative samples for each user.\n","\n","\\begin{equation}\n","L_{BPR} = -\\sum_{u = 1}^M \\sum_{i \\in N_u} \\sum_{j \\notin N_u} \\ln{\\sigma(\\hat{y}_{ui} - \\hat{y}_{uj})} + \\lambda ||E^{(0)}||^2 \n","\\end{equation}\n","\n","$\\hat{y}_{u}$: predicted score of a positive sample\n","\n","$\\hat{y}_{uj}$: predicted score of a negative sample\n","\n","$\\lambda$: hyperparameter which controls the L2 regularization strength"]},{"cell_type":"code","execution_count":null,"id":"ad40619e","metadata":{"id":"ad40619e"},"outputs":[],"source":["class BPR(nn.Module):\n","    def __init__(self, user_size, item_size, dim):\n","        super().__init__()\n","        self.user_embedding = nn.Embedding(user_size,dim)\n","        self.item_embedding = nn.Embedding(item_size,dim)\n","\n","    def forward(self, u, i, j):\n","        x_ui = torch.mul(self.user_embedding(u), self.item_embedding(i)).sum(dim=1)\n","        x_uj = torch.mul(self.user_embedding(u), self.item_embedding(j)).sum(dim=1)\n","        x_uij = x_ui - x_uj\n","        bpr_loss = -torch.log(torch.sigmoid(x_uij)).mean()\n","        return bpr_loss"]},{"cell_type":"code","execution_count":null,"id":"38d4531c","metadata":{"id":"38d4531c"},"outputs":[],"source":["def bpr_loss(user_embedding, pos_embedding, neg_embedding):\n","    \"\"\"Bayesian Personalized Ranking Loss as described in https://arxiv.org/abs/1205.2618\n","\n","    Args:\n","        user_embedding (torch.Tensor): user embedding\n","        pos_embedding (torch.Tensor): embedding of positive items\n","        neg_embedding (torch.Tensor): embedding of negative items\n","\n","    Returns:\n","        torch.Tensor: scalar bpr loss value\n","    \"\"\"\n","\n","    pos_scores = torch.mul(user_embedding, pos_embedding)\n","    pos_scores = torch.sum(pos_scores, dim=-1) # predicted scores of positive samples\n","    neg_scores = torch.mul(user_embedding, neg_embedding)\n","    neg_scores = torch.sum(neg_scores, dim=-1) # predicted scores of negative samples\n","\n","    loss = -torch.mean(torch.sigmoid(pos_scores - neg_scores))\n","\n","    return loss"]},{"cell_type":"code","execution_count":null,"id":"9fed3a2a","metadata":{"id":"9fed3a2a"},"outputs":[],"source":["from torch.utils.data import Dataset, DataLoader\n","\n","class TripletUniformPair(Dataset):\n","    def __init__(self, num_item, user_list, pair):\n","        self.num_item = num_item\n","        self.user_list = user_list\n","        self.pair = pair\n","\n","    def __getitem__(self, idx):\n","        idx = np.random.randint(len(self.pair))\n","        u = self.pair[idx][0]\n","        i = self.pair[idx][1]\n","        j = np.random.randint(self.num_item)\n","        while j in self.user_list[u]:\n","            j = np.random.randint(self.num_item)\n","        return u, i, j\n","\n","    def __len__(self):\n","        return len(self.pair)"]},{"cell_type":"markdown","id":"1b50a85c","metadata":{"id":"1b50a85c"},"source":["# Evaluation Metrics\n","\n","We evalaluate our model using the following metrics\n","\n","\\begin{equation}\n","\\text{Recall} = \\frac{TP}{TP + FP}\n","\\end{equation}\n","\n","\\begin{equation}\n","\\text{Precision} = \\frac{TP}{TP + FN}\n","\\end{equation}\n","\n","**Dicounted Cumulative Gain (DCG)** at rank position p is defined as:\n","\n","\\begin{equation}\n","\\text{DCG}_\\text{p} = \\sum_{i = 1}^p \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n","\\end{equation}\n","\n","p: a particular rank position\n","\n","$rel_i \\in \\{0, 1\\}$ : graded relevance of the result at position $i$\n","\n","**Idealised Dicounted Cumulative Gain (IDCG)**, namely the maximum possible DCG, at rank position $p$ is defined as:\n","\n","\\begin{equation}\n","\\text{IDCG}_\\text{p} = \\sum_{i = 1}^{|REL_p|} \\frac{2^{rel_i} - 1}{\\log_2{(i + 1)}}\n","\\end{equation}\n","\n","$|REL_p|$ : list of items ordered by their relevance up to position p\n","\n","**Normalized Dicounted Cumulative Gain (NDCG)** at rank position $p$ is defined as:\n","\n","\\begin{equation}\n","\\text{nDCG}_\\text{p} = \\frac{\\text{DCG}_p}{\\text{nDCG}_p}\n","\\end{equation}\n","\n","Specifically, we use the metrics recall@K, precision@K, and NDCG@K. @K indicates that these metrics are computed on the top K recommendations."]},{"cell_type":"code","execution_count":null,"id":"fffb4e33","metadata":{"id":"fffb4e33"},"outputs":[],"source":["# helper function to get N_u\n","def get_user_positive_items(interactions):\n","    \"\"\"Generates dictionary of positive items for each user\n","\n","    Args:\n","        interactions (torch.Tensor): N by 2 list of interaction\n","\n","    Returns:\n","        dict: dictionary of positive items for each user\n","    \"\"\"\n","    user_pos_items = {}\n","    for i in range(interactions.shape[0]):\n","        user = interactions[i][0].item()\n","        item = interactions[i][1].item()\n","        if user not in user_pos_items:\n","            user_pos_items[user] = []\n","        user_pos_items[user].append(item)\n","    return user_pos_items"]},{"cell_type":"code","execution_count":null,"id":"8fe36cb8","metadata":{"id":"8fe36cb8"},"outputs":[],"source":["# computes recall@K and precision@K\n","def RecallPrecision_ATk(groundTruth, r, k):\n","    \"\"\"Computers recall @ k and precision @ k\n","\n","    Args:\n","        groundTruth (list): list of lists containing highly rated items of each user\n","        r (list): list of lists indicating whether each top k item recommended to each user\n","            is a top k ground truth item or not\n","        k (intg): determines the top k items to compute precision and recall on\n","\n","    Returns:\n","        tuple: recall @ k, precision @ k\n","    \"\"\"\n","    num_correct_pred = torch.sum(r, dim=-1)  # number of correctly predicted items per user\n","    # number of items liked by each user in the test set\n","    user_num_liked = torch.Tensor([len(groundTruth[i])\n","                                  for i in range(len(groundTruth))])\n","    recall = torch.mean(num_correct_pred / user_num_liked)\n","    precision = torch.mean(num_correct_pred) / k\n","    return recall.item(), precision.item()"]},{"cell_type":"code","execution_count":null,"id":"54a08bd3","metadata":{"id":"54a08bd3"},"outputs":[],"source":["# computes NDCG@K\n","def NDCGatK_r(groundTruth, r, k):\n","    \"\"\"Computes Normalized Discounted Cumulative Gain (NDCG) @ k\n","\n","    Args:\n","        groundTruth (list): list of lists containing highly rated items of each user\n","        r (list): list of lists indicating whether each top k item recommended to each user\n","            is a top k ground truth item or not\n","        k (int): determines the top k items to compute ndcg on\n","\n","    Returns:\n","        float: ndcg @ k\n","    \"\"\"\n","    assert len(r) == len(groundTruth)\n","\n","    test_matrix = torch.zeros((len(r), k))\n","\n","    for i, items in enumerate(groundTruth):\n","        length = min(len(items), k)\n","        test_matrix[i, :length] = 1\n","    max_r = test_matrix\n","    idcg = torch.sum(max_r * 1. / torch.log2(torch.arange(2, k + 2)), axis=1)\n","    dcg = r * (1. / torch.log2(torch.arange(2, k + 2)))\n","    dcg = torch.sum(dcg, axis=1)\n","    idcg[idcg == 0.] = 1.\n","    ndcg = dcg / idcg\n","    ndcg[torch.isnan(ndcg)] = 0.\n","    return torch.mean(ndcg).item()"]},{"cell_type":"code","execution_count":null,"id":"eb3fcb5d","metadata":{"id":"eb3fcb5d"},"outputs":[],"source":["# wrapper function to get evaluation metrics\n","def get_metrics(model, interactions, exclude_interactions, k):\n","    \"\"\"\n","    Computes the evaluation metrics: recall, precision, and ndcg @ k\n","    \"\"\"\n","    user_embedding, item_embedding = model.user_embedding.weight, model.item_embedding.weight\n","\n","    # get ratings between every user and item - shape is num users x num movies\n","    rating = torch.matmul(user_embedding, item_embedding.T)\n","\n","    for exclude_record in exclude_interactions:\n","        # gets all the positive items for each user from the edge index\n","        user_pos_items = get_user_positive_items(exclude_record)\n","        # get coordinates of all edges to exclude\n","        exclude_users = []\n","        exclude_items = []\n","        for user, items in user_pos_items.items():\n","            exclude_users.extend([user] * len(items))\n","            exclude_items.extend(items)\n","\n","        # set ratings of excluded edges to large negative value\n","        rating[exclude_users, exclude_items] = -(1 << 10)\n","\n","    # get the top k recommended items for each user\n","    _, top_K_items = torch.topk(rating, k=k)\n","\n","    # get all unique users in evaluated split\n","    users = interactions[:,0].unique()\n","\n","    test_user_pos_items = get_user_positive_items(interactions)\n","\n","    # convert test user pos items dictionary into a list\n","    test_user_pos_items_list = [\n","        test_user_pos_items[user.item()] for user in users]\n","\n","    # determine the correctness of topk predictions\n","    r = []\n","    for user in users:\n","        ground_truth_items = test_user_pos_items[user.item()]\n","        label = list(map(lambda x: x in ground_truth_items, top_K_items[user]))\n","        r.append(label)\n","    r = torch.Tensor(np.array(r).astype('float'))\n","\n","    recall, precision = RecallPrecision_ATk(test_user_pos_items_list, r, k)\n","    ndcg = NDCGatK_r(test_user_pos_items_list, r, k)\n","\n","    return recall, precision, ndcg"]},{"cell_type":"code","execution_count":null,"id":"35863d6a","metadata":{"id":"35863d6a"},"outputs":[],"source":["# wrapper function to evaluate model\n","def evaluation(model, edge_index, exclude_edge_indices, k):\n","    \"\"\"\n","    Evaluates model loss and metrics including recall, precision, ndcg @ k\n","    \"\"\"\n","    # get embeddings\n","    users_emb_final, items_emb_final = model.user_embedding.weight, model.item_embedding.weight\n","    edges = structured_negative_sampling(\n","        edge_index.T, contains_neg_self_loops=False)\n","    \n","    # indices\n","    user_indices, pos_item_indices, neg_item_indices = edges[0], edges[1], edges[2]\n","    users_emb_final = users_emb_final[user_indices]\n","    pos_items_emb_final = items_emb_final[pos_item_indices]\n","    neg_items_emb_final = items_emb_final[neg_item_indices]\n","\n","    recall, precision, ndcg = get_metrics(\n","        model, edge_index, exclude_edge_indices, k)\n","\n","    return recall, precision, ndcg"]},{"cell_type":"markdown","id":"dd9e9e16","metadata":{"id":"dd9e9e16"},"source":["# Training"]},{"cell_type":"code","execution_count":null,"id":"ef072d84","metadata":{"id":"ef072d84"},"outputs":[],"source":["# define contants\n","ITERATIONS = 500\n","BATCH_SIZE = 1024\n","LR = 1e-3\n","ITERS_PER_EVAL = 100\n","ITERS_PER_LR_DECAY = 200\n","K = 20\n","LAMBDA = 1e-6\n","DIM = 32"]},{"cell_type":"code","execution_count":null,"id":"30fc7342","metadata":{"id":"30fc7342"},"outputs":[],"source":["# setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device {device}.\")\n","\n","model = BPR(num_users, num_movies, DIM)\n","model = model.to(device)\n","model.train()\n","\n","# initialize parameters\n","for p in model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=LAMBDA)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","\n","user_interaction_list = get_user_positive_items(train_interactions)\n","train_dataset = TripletUniformPair(num_movies, user_interaction_list, train_interactions.numpy())\n","train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE,pin_memory=True,num_workers=8)"]},{"cell_type":"code","execution_count":null,"id":"9ba816b4","metadata":{"id":"9ba816b4"},"outputs":[],"source":["from tqdm.autonotebook import tqdm, trange"]},{"cell_type":"code","execution_count":null,"id":"3340ff8a","metadata":{"scrolled":true,"id":"3340ff8a"},"outputs":[],"source":["# training loop\n","for iter in trange(ITERATIONS):\n","    for batch in train_loader:\n","        batch = [item.to(device) for item in batch]\n","        train_loss = model(*batch)\n","        optimizer.zero_grad()\n","        train_loss.backward()\n","        optimizer.step()\n","        \n","    if iter % ITERS_PER_EVAL == 0:\n","        model.eval()\n","        recall, precision, ndcg = evaluation(model, val_interactions, [train_interactions], K)\n","        print(f\"[Iteration {iter}/{ITERATIONS}] val_recall@{K}: {recall:.4f}, val_precision@{K}: {precision:.4f}, val_ndcg@{K}: {ndcg:.4f}\")\n","        model.train()\n","\n","    if iter % ITERS_PER_LR_DECAY == 0 and iter != 0:\n","        scheduler.step()"]},{"cell_type":"code","execution_count":null,"id":"9b8ecada","metadata":{"id":"9b8ecada"},"outputs":[],"source":["# evaluate on test set\n","model.eval()\n","\n","test_recall, test_precision, test_ndcg = evaluation(\n","            model, test_interactions, [train_interactions, val_interactions], K)\n","\n","print(f\"test_recall@{K}: {round(test_recall, 5)}, test_precision@{K}: {round(test_precision, 5)}, test_ndcg@{K}: {round(test_ndcg, 5)}\")"]},{"cell_type":"markdown","id":"3e0e3c0a","metadata":{"id":"3e0e3c0a"},"source":["# Make New Recommendatios for a Given User"]},{"cell_type":"code","execution_count":null,"id":"0fdea58d","metadata":{"id":"0fdea58d"},"outputs":[],"source":["model.eval()\n","df = pd.read_csv(movie_path)\n","movieid_title = pd.Series(df.title.values,index=df.movieId).to_dict()\n","movieid_genres = pd.Series(df.genres.values,index=df.movieId).to_dict()\n","\n","user_pos_items = get_user_positive_items(interactions)"]},{"cell_type":"code","execution_count":null,"id":"33bbb1a3","metadata":{"id":"33bbb1a3"},"outputs":[],"source":["def make_predictions(user_id, num_recs):\n","    user = user_mapping[user_id]\n","    e_u = model.user_embedding.weight[user]\n","    scores = model.item_embedding.weight @ e_u\n","\n","    values, indices = torch.topk(scores, k=len(user_pos_items[user]) + num_recs)\n","\n","    movies = [index.cpu().item() for index in indices if index in user_pos_items[user]][:num_recs]\n","    movie_ids = [list(movie_mapping.keys())[list(movie_mapping.values()).index(movie)] for movie in movies]\n","    titles = [movieid_title[id] for id in movie_ids]\n","    genres = [movieid_genres[id] for id in movie_ids]\n","\n","    print(f\"Here are some movies that user {user_id} rated highly\")\n","    for i in range(num_recs):\n","        print(f\"title: {titles[i]}, genres: {genres[i]} \")\n","\n","    print()\n","\n","    movies = [index.cpu().item() for index in indices if index not in user_pos_items[user]][:num_recs]\n","    movie_ids = [list(movie_mapping.keys())[list(movie_mapping.values()).index(movie)] for movie in movies]\n","    titles = [movieid_title[id] for id in movie_ids]\n","    genres = [movieid_genres[id] for id in movie_ids]\n","\n","    print(f\"Here are some suggested movies for user {user_id}\")\n","    for i in range(num_recs):\n","        print(f\"title: {titles[i]}, genres: {genres[i]} \")"]},{"cell_type":"code","execution_count":null,"id":"17e3f97c","metadata":{"id":"17e3f97c"},"outputs":[],"source":["USER_ID = 30\n","NUM_RECS = 10\n","\n","make_predictions(USER_ID, NUM_RECS)"]},{"cell_type":"markdown","id":"5bf022b6","metadata":{"id":"5bf022b6"},"source":["# Colaborative filtering with NeuMF\n","Neural Collaborative Filtering(NCF) replaces the user-item inner product with a neural architecture. By doing so NCF tried to achieve the following:\n","\n","* NCF tries to express and generalize MF under its framework.\n","* NCF tries to learn User-item interactions through a multi-layer perceptron."]},{"cell_type":"markdown","id":"94a7332c","metadata":{"id":"94a7332c"},"source":["## Generalize Matrix Factorization\n","The predicted output of the GMF can be expressed as\n","\n","![](https://miro.medium.com/max/1196/1*oLGMj-8x7WLectRAk20ZVA.png)\n","where\n","* $a_{out}$: activation function\n","* $h$: weights of the output layer\n","![](https://miro.medium.com/max/1400/1*dwXGkFZCbzhRR0blfgQkVA.png)\n","\n","As you can see from the above table that GMF with identity activation function and edge weights as 1 is indeed MF. The other 2 variations are expansions on the generic MF. The last variation of GMF with sigmoid as activation is used in NCF."]},{"cell_type":"code","execution_count":null,"id":"1bb29eda","metadata":{"id":"1bb29eda"},"outputs":[],"source":["class GMF(nn.Module):\n","    def __init__(self, user_size, item_size, dim):\n","        super().__init__()\n","        self.user_embedding = nn.Embedding(user_size,dim)\n","        self.item_embedding = nn.Embedding(item_size,dim)\n","        self.output_layer = nn.Linear(dim,1,bias=False)\n","\n","    def forward(self, user_id, item_id):\n","        user_vectors = self.user_embedding(user_id)\n","        item_vectors = self.item_embedding(item_id)\n","        \n","        # interaction\n","        interactions = user_vectors * item_vectors\n","        \n","        # predictions\n","        predictions = self.output_layer(interactions)\n","        \n","        return predictions.flatten()"]},{"cell_type":"code","execution_count":null,"id":"245d8c1a","metadata":{"id":"245d8c1a"},"outputs":[],"source":["import scipy.sparse as sp\n","# load ratings as a dok matrix\n","train_mat = sp.dok_matrix((num_users, num_movies), dtype=np.float32)\n","for x in train_interactions:\n","    train_mat[x[0], x[1]] = 1.0"]},{"cell_type":"code","execution_count":null,"id":"c627c90b","metadata":{"id":"c627c90b"},"outputs":[],"source":["class NCFData(Dataset):\n","    def __init__(self, features, \n","                num_item, train_mat=None, num_ng=0, is_training=None):\n","        super(NCFData, self).__init__()\n","        \"\"\" Note that the labels are only useful when training, we thus \n","            add them in the ng_sample() function.\n","        \"\"\"\n","        self.features_ps = features\n","        self.num_item = num_item\n","        self.train_mat = train_mat\n","        self.num_ng = num_ng\n","        self.is_training = is_training\n","        self.labels = [0 for _ in range(len(features))]\n","\n","    def ng_sample(self):\n","        assert self.is_training, 'no need to sampling when testing'\n","\n","        self.features_ng = []\n","        for x in self.features_ps:\n","            u = x[0]\n","            for t in range(self.num_ng):\n","                j = np.random.randint(self.num_item)\n","                while (u, j) in self.train_mat:\n","                    j = np.random.randint(self.num_item)\n","                self.features_ng.append([u, j])\n","\n","        labels_ps = [1 for _ in range(len(self.features_ps))]\n","        labels_ng = [0 for _ in range(len(self.features_ng))]\n","\n","        self.features_fill = self.features_ps + self.features_ng\n","        self.labels_fill = labels_ps + labels_ng\n","\n","    def __len__(self):\n","        return (self.num_ng + 1) * len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        features = self.features_fill if self.is_training \\\n","                    else self.features_ps\n","        labels = self.labels_fill if self.is_training \\\n","                    else self.labels\n","\n","        user = features[idx][0]\n","        item = features[idx][1]\n","        label = labels[idx]\n","        return user, item ,label"]},{"cell_type":"code","execution_count":null,"id":"afd41d0b","metadata":{"id":"afd41d0b"},"outputs":[],"source":["def hit(gt_item, pred_items):\n","    if gt_item in pred_items:\n","        return 1\n","    return 0\n","\n","\n","def ndcg(gt_item, pred_items):\n","    if gt_item in pred_items:\n","        index = pred_items.index(gt_item)\n","        return np.reciprocal(np.log2(index+2))\n","    return 0\n","\n","\n","def metrics(model, test_loader, top_k):\n","    HR, NDCG = [], []\n","\n","    for user, item, label in test_loader:\n","        user = user.cuda()\n","        item = item.cuda()\n","\n","        predictions = model(user, item)\n","        _, indices = torch.topk(predictions, top_k)\n","        recommends = torch.take(\n","                item, indices).cpu().numpy().tolist()\n","\n","        gt_item = item[0].item()\n","        HR.append(hit(gt_item, recommends))\n","        NDCG.append(ndcg(gt_item, recommends))\n","\n","    return np.mean(HR), np.mean(NDCG)"]},{"cell_type":"code","execution_count":null,"id":"5bdf8575","metadata":{"id":"5bdf8575"},"outputs":[],"source":["# construct the train and test datasets\n","BATCH_SIZE = 64\n","train_dataset = NCFData(\n","    train_interactions.tolist(), num_movies, train_mat, 1, True)\n","test_dataset = NCFData(\n","        test_interactions.tolist(), num_movies, train_mat, 0, False)\n","train_loader = DataLoader(train_dataset,\n","        batch_size=BATCH_SIZE, shuffle=True, num_workers=4)\n","test_loader = DataLoader(test_dataset,\n","        batch_size=100, shuffle=False, num_workers=0)"]},{"cell_type":"code","execution_count":null,"id":"18a0096e","metadata":{"id":"18a0096e"},"outputs":[],"source":["# define contants\n","ITERATIONS = 20\n","BATCH_SIZE = 256\n","LR = 1e-3\n","K = 20\n","LAMBDA = 0\n","DIM = 16\n","\n","# setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device {device}.\")\n","\n","model = GMF(num_users, num_movies, DIM)\n","model = model.to(device)\n","model.train()\n","\n","# initialize parameters\n","for p in model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=LAMBDA)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.99)\n","objective_function = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":null,"id":"c14cb842","metadata":{"scrolled":true,"id":"c14cb842"},"outputs":[],"source":["# training loop\n","for iter in trange(ITERATIONS):\n","    train_loader.dataset.ng_sample()\n","    for batch in train_loader:\n","        batch = [item.to(device) for item in batch]\n","        user, item, label = batch\n","        predictions = model(user,item)\n","        loss = objective_function(predictions,label.float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    # evaluation\n","    model.eval()\n","    HR, NDCG = metrics(model, test_loader, K)\n","    print(\"HR@{}: {:.3f}  NDCG@{}: {:.3f}\".format(K,np.mean(HR),K, np.mean(NDCG)))"]},{"cell_type":"markdown","id":"4d4129fe","metadata":{"id":"4d4129fe"},"source":["## MLP in NCF\n","NCF is an example of multimodal deep learning as it contains data from 2 pathways namely user and item. The most intuitive way to combine them is by concatenation. But a simple vector concatenation does not account for user-item interactions and is insufficient to model the collaborative filtering effect. To address this NCF adds hidden layers on top of concatenated user-item vectors(MLP framework), to learn user-item interactions. This endows the model with a lot of flexibility and non-linearity to learn the user-item interactions. This is an upgrade over MF that uses a fixed element-wise product on them. More precisely, the MLP alter Equation 1 as follows\n","![](https://miro.medium.com/max/1252/1*tIQfBeTur0gaKObfvDXmpw.png)\n","![](https://miro.medium.com/max/1400/1*aP-Mx266ExwoWZPSdHtYpA.png)"]},{"cell_type":"code","execution_count":null,"id":"810ecaa1","metadata":{"id":"810ecaa1"},"outputs":[],"source":["class MLP(nn.Module):\n","    def __init__(self, user_size, item_size, dim):\n","        super().__init__()\n","        self.user_embedding = nn.Embedding(user_size,dim*2)\n","        self.item_embedding = nn.Embedding(item_size,dim*2)\n","        self.MLP = nn.Sequential(\n","            nn.Linear(dim*4,dim*2),\n","            nn.ReLU(),\n","            nn.Linear(dim*2,dim),\n","            nn.ReLU(),\n","            nn.Linear(dim,1),\n","        )\n","\n","    def forward(self, user_id, item_id):\n","        user_vectors = self.user_embedding(user_id)\n","        item_vectors = self.item_embedding(item_id)\n","        \n","        # interaction\n","        interactions = torch.cat([user_vectors, item_vectors],dim=-1)\n","        \n","        # predictions\n","        predictions = self.MLP(interactions)\n","        \n","        return predictions.flatten()"]},{"cell_type":"code","execution_count":null,"id":"7fce51ff","metadata":{"id":"7fce51ff"},"outputs":[],"source":["# define contants\n","ITERATIONS = 50\n","LR = 1e-4\n","K = 20\n","LAMBDA = 0\n","DIM = 16\n","\n","# setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device {device}.\")\n","\n","model = MLP(num_users, num_movies, DIM)\n","model = model.to(device)\n","model.train()\n","\n","# initialize parameters\n","for p in model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=LAMBDA)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","objective_function = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":null,"id":"40cec0fc","metadata":{"scrolled":true,"id":"40cec0fc"},"outputs":[],"source":["# training loop\n","for iter in trange(ITERATIONS):\n","    train_loader.dataset.ng_sample()\n","    for batch in train_loader:\n","        batch = [item.to(device) for item in batch]\n","        user, item, label = batch\n","        predictions = model(user,item)\n","        loss = objective_function(predictions,label.float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    # evaluation\n","    model.eval()\n","    HR, NDCG = metrics(model, test_loader, K)\n","    print(\"HR@{}: {:.3f}  NDCG@{}: {:.3f}\".format(K,np.mean(HR),K, np.mean(NDCG)))"]},{"cell_type":"markdown","id":"e2519c97","metadata":{"id":"e2519c97"},"source":["# Practice: implementing NeuMF that combines GMF and MLP\n","\n","![](https://miro.medium.com/max/1400/1*Tqk7Q2q7wsr6MLF8Xl-emg.png)"]},{"cell_type":"code","execution_count":null,"id":"703eb6c6","metadata":{"id":"703eb6c6"},"outputs":[],"source":["class NeuMF(nn.Module):\n","    def __init__(self, user_size, item_size, dim):\n","        super().__init__()\n","        ############################################################################\n","        # TODO: Your code here!\n","        # create embeddings and layers from GMF and MLP\n","\n","        ############################################################################\n","        # an output layer that transform concat vector to prediction\n","        self.output_layer = nn.Linear(dim*2,1,bias=False)\n","\n","    def forward(self, user_id, item_id):\n","        MLP_feature = None\n","        GMF_feature = None\n","        ############################################################################\n","        # TODO: Your code here!\n","        # please obtain the embedding from MLP and GMF\n","        # store the embedding in MLP_feature and GMF_feature, respectively.\n","        \n","        ############################################################################\n","        \n","        # predictions\n","        features = torch.cat([MLP_feature,GMF_feature],dim=-1)\n","        predictions = self.output_layer(features)\n","        \n","        return predictions.flatten()"]},{"cell_type":"code","execution_count":null,"id":"889675ba","metadata":{"id":"889675ba"},"outputs":[],"source":["# define contants\n","\n","\n","# setup\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Using device {device}.\")\n","\n","model = NeuMF(num_users, num_movies, DIM).to(device)\n","model.train()\n","\n","# initialize parameters\n","for p in model.parameters():\n","    if p.dim() > 1:\n","        nn.init.xavier_uniform_(p)\n","\n","optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=LAMBDA)\n","scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.95)\n","objective_function = nn.BCEWithLogitsLoss()"]},{"cell_type":"markdown","id":"ca6451c1","metadata":{"id":"ca6451c1"},"source":["## Training!\n","Let's see if NeuMF outperforms GMF and MLP?"]},{"cell_type":"code","execution_count":null,"id":"27e3bf88","metadata":{"scrolled":true,"id":"27e3bf88"},"outputs":[],"source":["# training loop\n","for iter in trange(ITERATIONS):\n","    train_loader.dataset.ng_sample()\n","    for batch in train_loader:\n","        batch = [item.to(device) for item in batch]\n","        user, item, label = batch\n","        predictions = model(user,item)\n","        loss = objective_function(predictions,label.float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    # evaluation\n","    model.eval()\n","    HR, NDCG = metrics(model, test_loader, K)\n","    print(\"HR@{}: {:.3f}  NDCG@{}: {:.3f}\".format(K,np.mean(HR),K, np.mean(NDCG)))"]},{"cell_type":"code","execution_count":null,"id":"2c7caf42","metadata":{"id":"2c7caf42"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"pyg","language":"python","name":"pyg"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}