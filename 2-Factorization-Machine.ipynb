{"cells":[{"cell_type":"markdown","id":"4b4c46f4","metadata":{"id":"4b4c46f4"},"source":["# Factorization Machine\n","In this article, we‚Äôll introduce Factorization Machines (FM) as a flexible and powerful modeling framework for collaborative filtering recommendation. <br>\n","Specifically, we will demonstrate how FM achieves second-order interaction which improves the performance on top of the first-order model.<br>\n","\n","\n","FM is an effective method to handle data with high dimensionality categorical data like the following figure.\n","![](https://miro.medium.com/max/1400/1*MKqlUaZGiECvqTOJElmfhg.png)"]},{"cell_type":"code","execution_count":null,"id":"773d3d20","metadata":{"id":"773d3d20"},"outputs":[],"source":["# install data\n","! wget https://zenodo.org/record/5700987/files/Criteo_x1.zip\n","! unzip Criteo_x1.zip -d Criteo"]},{"cell_type":"code","execution_count":null,"id":"e7dba2e8","metadata":{"id":"e7dba2e8"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","\n","from torch.utils.data import DataLoader,Dataset\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","execution_count":null,"id":"8f3248cc","metadata":{"id":"8f3248cc"},"outputs":[],"source":["# data = pd.read_csv('./criteo_sample.txt')\n","data = pd.read_csv('./Criteo/train.csv',nrows=1e5)\n","\n","# separate numerical and categorical features\n","sparse_features = ['C' + str(i) for i in range(1, 27)]\n","dense_features = ['I' + str(i) for i in range(1, 14)]\n","\n","data.head(3)"]},{"cell_type":"markdown","id":"d7460b64","metadata":{"id":"d7460b64"},"source":["Let's take a look at the number of unique categorical variables in each column"]},{"cell_type":"code","execution_count":null,"id":"1630cdd9","metadata":{"id":"1630cdd9"},"outputs":[],"source":["# preprocess\n","data[sparse_features] = data[sparse_features].fillna('-1', )\n","data = data.drop(dense_features,axis=1)\n","data = data.apply(lambda x:LabelEncoder().fit_transform(x))\n","targets = data['label']\n","data = data.drop(\"label\",axis=1)\n","\n","# calculate field dimensions\n","field_dims = data.nunique().values\n","print(field_dims)"]},{"cell_type":"code","execution_count":null,"id":"95f20074","metadata":{"id":"95f20074"},"outputs":[],"source":["class CriteoDataset(Dataset):\n","    def __init__(self, num_fields, features, target):\n","        super().__init__()\n","        self.num_fields = num_fields\n","        self.features = torch.LongTensor(features)\n","        self.target = torch.FloatTensor(target)\n","        \n","    def __getitem__(self,index):\n","        return self.features[index,:], self.target[index]\n","    \n","    def __len__(self):\n","        return len(self.features)"]},{"cell_type":"code","execution_count":null,"id":"214a00e1","metadata":{"id":"214a00e1"},"outputs":[],"source":["dataset = CriteoDataset(field_dims, data.values, targets.values)\n","\n","# split dataset\n","train_length = int(len(dataset) * 0.8)\n","valid_length = int(len(dataset) * 0.1)\n","test_length = len(dataset) - train_length - valid_length\n","train_dataset, valid_dataset, test_dataset = torch.utils.data.random_split(\n","        dataset, (train_length, valid_length, test_length))\n","\n","# loader\n","batch_size = 512\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers=2,shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=2)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers=2)"]},{"cell_type":"markdown","id":"f088ebde","metadata":{"id":"f088ebde"},"source":["# From first-order to second-order interaction\n","\n","## First-order model: Logistic regression\n","Normally, when we think of linear regression, we would think of the following formula:\n","$$\n","\\hat{y}(x) = \\mathbf{w}_0 + \\sum_{i=1}^d \\mathbf{w}_i x_i\n","$$\n","Where:\n","$\\mathbf{w}_0$  is the bias term, a.k.a intercept.\n","$\\mathbf{w}_i$  are weights corresponding to each feature vector $x_i$ , here we assume we have  $n$  total features.\n","This formula's advantage is that it can computed in linear time, $ùëÇ(ùëõ)$ . The drawback, however, is that it does not handle feature interactions. <br>\n","Let's first implement the LR model and see how it works!"]},{"cell_type":"code","execution_count":null,"id":"366af835","metadata":{"id":"366af835"},"outputs":[],"source":["class LogisticRegression(torch.nn.Module):\n","    def __init__(self, field_dims, output_dim=1):\n","        super().__init__()\n","        self.fc = torch.nn.Embedding(sum(field_dims), output_dim)\n","        self.bias = torch.nn.Parameter(torch.zeros((output_dim,)))\n","        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: Long tensor of size ``(batch_size, num_fields)``\n","        \"\"\"\n","        # Since the value of each dimension starts with 0\n","        # we need to add the offsets of previous dimensions\n","        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n","        \n","        # first-order term (i.e. \\sum w_i x_i)\n","        first_order = torch.sum(self.fc(x), dim=1)\n","        \n","        # zero-order (i.e. bias)\n","        zero_order = self.bias\n","        \n","        output = first_order + zero_order\n","        \n","        return output.flatten()"]},{"cell_type":"code","execution_count":null,"id":"9b602b0f","metadata":{"id":"9b602b0f"},"outputs":[],"source":["from sklearn.metrics import roc_auc_score\n","def test(model, data_loader, device):\n","    model.eval()\n","    targets, predicts = list(), list()\n","    with torch.no_grad():\n","        for fields, target in data_loader:\n","            fields, target = fields.to(device), target.to(device)\n","            y = model(fields)\n","            targets.extend(target.tolist())\n","            predicts.extend(y.tolist())\n","    return roc_auc_score(targets, predicts)"]},{"cell_type":"code","execution_count":null,"id":"9c1fb24a","metadata":{"id":"9c1fb24a"},"outputs":[],"source":["# training configs\n","EPOCHS = 10\n","lr = 1e-4\n","device = \"cuda\"\n","\n","# build model\n","model = LogisticRegression(field_dims).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","objective_function = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":null,"id":"e6d9aaac","metadata":{"id":"e6d9aaac"},"outputs":[],"source":["for epoch in range(EPOCHS):\n","    for fields, target in train_loader:\n","        fields, target = fields.to(device), target.to(device)\n","        prediction = model(fields)\n","        loss = objective_function(prediction, target.float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    valid_roc = test(model, valid_loader,device)\n","    print(f\"Validation ROC score:{valid_roc:.4f}\")\n","    \n","test_roc = test(model, test_loader,device)\n","print(f\"Testing ROC score:{test_roc:.4f}\")"]},{"cell_type":"markdown","id":"3c4bcc97","metadata":{"id":"3c4bcc97"},"source":["# Factorization Machines\n","\n","Factorization machines (FM) `Rendle.2010`, proposed by Steffen Rendle in 2010, is a supervised algorithm that can be used for classification, regression, and ranking tasks. It quickly took notice and became a popular and impactful method for making predictions and recommendations. Particularly, it is a generalization of the linear regression model and the matrix factorization model. Moreover, it is reminiscent of support vector machines with a polynomial kernel. The strengths of factorization machines over the linear regression and matrix factorization are: (1) it can model $\\chi$-way variable interactions, where $\\chi$ is the number of polynomial order and is usually set to two. (2) A fast optimization algorithm associated with factorization machines can reduce the polynomial computation time to linear complexity, making it extremely efficient especially for high dimensional sparse inputs.  For these reasons, factorization machines are widely employed in modern advertisement and products recommendations. The technical details and implementations are described below.\n","\n","\n","## 2-Way Factorization Machines\n","\n","Formally, let $x \\in \\mathbb{R}^d$ denote the feature vectors of one sample, and $y$ denote the corresponding label which can be real-valued label or class label such as binary class \"click/non-click\". The model for a factorization machine of degree two is defined as:\n","\n","$$\n","\\hat{y}(x) = \\mathbf{w}_0 + \\sum_{i=1}^d \\mathbf{w}_i x_i + \\sum_{i=1}^d\\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j\n","$$\n","\n","where $\\mathbf{w}_0 \\in \\mathbb{R}$ is the global bias; $\\mathbf{w} \\in \\mathbb{R}^d$ denotes the weights of the i-th variable; $\\mathbf{V} \\in \\mathbb{R}^{d\\times k}$ represents the feature embeddings; $\\mathbf{v}_i$ represents the $i^\\mathrm{th}$ row of $\\mathbf{V}$; $k$ is the dimensionality of latent factors; $\\langle\\cdot, \\cdot \\rangle$ is the dot product of two vectors.  $\\langle \\mathbf{v}_i, \\mathbf{v}_j \\rangle$ model the interaction between the $i^\\mathrm{th}$ and $j^\\mathrm{th}$ feature. Some feature interactions can be easily understood so they can be designed by experts. However, most other feature interactions are hidden in data and difficult to identify. So modeling feature interactions automatically can greatly reduce the efforts in feature engineering. It is obvious that the first two terms correspond to the linear regression model and the last term is an extension of the matrix factorization model. If the feature $i$ represents an item and the feature $j$ represents a user, the third term is exactly the dot product between user and item embeddings. It is worth noting that FM can also generalize to higher orders (degree > 2). Nevertheless, the numerical stability might weaken the generalization.\n","\n","\n","## An Efficient Optimization Criterion\n","\n","Optimizing the factorization machines in a  straight forward method leads to a complexity of $\\mathcal{O}(kd^2)$ as all pairwise interactions require to be computed. To solve this inefficiency problem, we can reorganize the third term of FM which could greatly reduce the computation cost, leading to a linear time complexity ($\\mathcal{O}(kd)$).  The reformulation of the pairwise interaction term is as follows:\n","\n","$$\n","\\begin{aligned}\n","&\\sum_{i=1}^d \\sum_{j=i+1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j \\\\\n"," &= \\frac{1}{2} \\sum_{i=1}^d \\sum_{j=1}^d\\langle\\mathbf{v}_i, \\mathbf{v}_j\\rangle x_i x_j - \\frac{1}{2}\\sum_{i=1}^d \\langle\\mathbf{v}_i, \\mathbf{v}_i\\rangle x_i x_i \\\\\n"," &= \\frac{1}{2} \\big (\\sum_{i=1}^d \\sum_{j=1}^d \\sum_{l=1}^k\\mathbf{v}_{i, l} \\mathbf{v}_{j, l} x_i x_j - \\sum_{i=1}^d \\sum_{l=1}^k \\mathbf{v}_{i, l} \\mathbf{v}_{i, l} x_i x_i \\big)\\\\\n"," &=  \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i) (\\sum_{j=1}^d \\mathbf{v}_{j, l}x_j) - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2 \\big ) \\\\\n"," &= \\frac{1}{2} \\sum_{l=1}^k \\big ((\\sum_{i=1}^d \\mathbf{v}_{i, l} x_i)^2 - \\sum_{i=1}^d \\mathbf{v}_{i, l}^2 x_i^2)\n"," \\end{aligned}\n","$$\n","\n","With this reformulation, the model complexity are decreased greatly. Moreover, for sparse features, only non-zero elements needs to be computed so that the overall complexity is linear to the number of non-zero features.\n","\n","To learn the FM model, we can use the MSE loss for regression task, the cross-entropy loss for classification tasks, and the BPR loss for ranking task. Standard optimizers such as stochastic gradient descent and Adam are viable for optimization.\n"]},{"cell_type":"code","execution_count":null,"id":"624eaf4e","metadata":{"id":"624eaf4e"},"outputs":[],"source":["class FeaturesEmbedding(torch.nn.Module):\n","    def __init__(self, field_dims, embed_dim):\n","        super().__init__()\n","        self.embedding = torch.nn.Embedding(sum(field_dims), embed_dim)\n","        self.offsets = np.array((0, *np.cumsum(field_dims)[:-1]), dtype=np.long)\n","        torch.nn.init.xavier_uniform_(self.embedding.weight.data)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: Long tensor of size ``(batch_size, num_fields)``\n","        \"\"\"\n","        x = x + x.new_tensor(self.offsets).unsqueeze(0)\n","        return self.embedding(x)\n","    \n","class FactorizationMachine(torch.nn.Module):\n","    def __init__(self, field_dims, embed_dim):\n","        super().__init__()\n","        self.linear = LogisticRegression(field_dims)\n","        self.feature_embedding = FeaturesEmbedding(field_dims, embed_dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: Long tensor of size ``(batch_size, num_fields)``\n","        \"\"\"\n","        # zero+first order (linear)\n","        linear_predictions = self.linear(x)\n","        \n","        # second-order interaction\n","        embedding_feautre = self.feature_embedding(x)\n","        square_of_sum = torch.sum(embedding_feautre, dim=1) ** 2\n","        sum_of_square = torch.sum(embedding_feautre ** 2, dim=1)\n","        fm_predictions = square_of_sum - sum_of_square\n","        fm_predictions = 0.5 * torch.sum(fm_predictions, dim=1, keepdim=True).flatten()\n","        return fm_predictions + linear_predictions\n","        "]},{"cell_type":"code","execution_count":null,"id":"12ce537e","metadata":{"id":"12ce537e"},"outputs":[],"source":["# training configs\n","EPOCHS = 10\n","lr = 1e-4\n","DIM = 16\n","device = \"cuda\"\n","\n","# build model\n","model = FactorizationMachine(field_dims,embed_dim=DIM).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","objective_function = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":null,"id":"80d674a3","metadata":{"id":"80d674a3"},"outputs":[],"source":["for epoch in range(EPOCHS):\n","    for fields, target in train_loader:\n","        fields, target = fields.to(device), target.to(device)\n","        prediction = model(fields)\n","        loss = objective_function(prediction, target.float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    valid_roc = test(model, valid_loader,device)\n","    print(f\"Validation ROC score:{valid_roc:.4f}\")\n","    \n","test_roc = test(model, test_loader,device)\n","print(f\"Testing ROC score:{test_roc:.4f}\")"]},{"cell_type":"markdown","id":"46b2ef85","metadata":{"id":"46b2ef85"},"source":["# From second-order to higher-order interaction\n","\n","## Deep Factorization Machines\n","\n","Learning effective feature combinations is critical to the success of click-through rate prediction task. Factorization machines model feature interactions in a linear paradigm (e.g., bilinear interactions). This is often insufficient for real-world data where inherent feature crossing structures are usually very complex and nonlinear. What's worse, second-order feature interactions are generally used in factorization machines in practice. Modeling higher degrees of feature combinations with factorization machines is possible theoretically but it is usually not adopted due to numerical instability and high computational complexity.\n","\n","One effective solution is using deep neural networks. Deep neural networks are powerful in feature representation learning and have the potential to learn sophisticated feature interactions. As such, it is natural to integrate deep neural networks to factorization machines. Adding nonlinear transformation layers to factorization machines gives it the capability to model both low-order feature combinations and high-order feature combinations. Moreover, non-linear inherent structures from inputs can also be captured with deep neural networks. In this section, we will introduce a representative model named deep factorization machines (DeepFM) `Guo.Tang.Ye.ea.2017` which combine FM and deep neural networks.\n","\n","\n","## Model Architectures\n","\n","DeepFM consists of an FM component and a deep component which are integrated in a parallel structure. The FM component is the same as the 2-way factorization machines which is used to model the low-order feature interactions. The deep component is an MLP that is used to capture high-order feature interactions and nonlinearities. These two components share the same inputs/embeddings and their outputs are summed up as the final prediction. It is worth pointing out that the spirit of DeepFM resembles that of the Wide \\& Deep architecture which can capture both memorization and generalization. The advantages of DeepFM over the Wide \\& Deep model is that it reduces the effort of hand-crafted feature engineering by identifying feature combinations automatically.\n","\n","We omit the description of the FM component for brevity and denote the output as $\\hat{y}^{(FM)}$. Readers are referred to the last section for more details. Let $\\mathbf{e}_i \\in \\mathbb{R}^{k}$ denote the latent feature vector of the $i^\\mathrm{th}$ field.  The input of the deep component is the concatenation of the dense embeddings of all fields that are looked up with the sparse categorical feature input, denoted as:\n","\n","$$\n","\\mathbf{z}^{(0)}  = [\\mathbf{e}_1, \\mathbf{e}_2, ..., \\mathbf{e}_f],\n","$$\n","\n","where $f$ is the number of fields.  It is then fed into the following neural network:\n","\n","$$\n","\\mathbf{z}^{(l)}  = \\alpha(\\mathbf{W}^{(l)}\\mathbf{z}^{(l-1)} + \\mathbf{b}^{(l)}),\n","$$\n","\n","where $\\alpha$ is the activation function.  $\\mathbf{W}_{l}$ and $\\mathbf{b}_{l}$ are the weight and bias at the $l^\\mathrm{th}$ layer. Let $y_{DNN}$ denote the output of the prediction. The ultimate prediction of DeepFM is the summation of the outputs from both FM and DNN. So we have:\n","\n","$$\n","\\hat{y} = \\sigma(\\hat{y}^{(FM)} + \\hat{y}^{(DNN)}),\n","$$\n","\n","where $\\sigma$ is the sigmoid function. The architecture of DeepFM is illustrated below.\n","![Illustration of the DeepFM model](https://raw.githubusercontent.com/d2l-ai/d2l-en-colab/master/img/rec-deepfm.svg)\n","\n","It is worth noting that DeepFM is not the only way to combine deep neural networks with FM. We can also add nonlinear layers over the feature interactions `He.Chua.2017`.\n"]},{"cell_type":"code","execution_count":null,"id":"1461801e","metadata":{"id":"1461801e"},"outputs":[],"source":["class MLP(torch.nn.Module):\n","\n","    def __init__(self, input_dim, embed_dims, dropout, output_layer=True):\n","        super().__init__()\n","        layers = list()\n","        for embed_dim in embed_dims:\n","            layers.append(torch.nn.Linear(input_dim, embed_dim))\n","            layers.append(torch.nn.BatchNorm1d(embed_dim))\n","            layers.append(torch.nn.ReLU())\n","            layers.append(torch.nn.Dropout(p=dropout))\n","            input_dim = embed_dim\n","        if output_layer:\n","            layers.append(torch.nn.Linear(input_dim, 1))\n","        self.mlp = torch.nn.Sequential(*layers)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        :param x: Float tensor of size ``(batch_size, embed_dim)``\n","        \"\"\"\n","        return self.mlp(x).flatten()"]},{"cell_type":"markdown","id":"3697ebec","metadata":{"id":"3697ebec"},"source":["## Practice: implementing DeepFM\n","In this exercise, you need to implement the DeepFM model on your own.<br>\n","Specifically, DeepFM consists of two parts: FM and MLP.<br>\n","* FM: The same architecture as we described previously.\n","* MLP: The field embeddings are concatenated and passed into a MLP for learning higher-order interactions.\n","\n","\n","Based on that, please implement the DeepFM model accordingly. We provide a sketch of MLP module above."]},{"cell_type":"code","execution_count":null,"id":"46dc00b8","metadata":{"id":"46dc00b8"},"outputs":[],"source":["class DeepFM(torch.nn.Module):\n","    def __init__(self, field_dims, embed_dim, mlp_dims, dropout):\n","        super().__init__()\n","        ############################################################################\n","        # TODO: Your code here!\n","        # define the necessary modules here\n","\n","        ############################################################################\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: Long tensor of size ``(batch_size, num_fields)``\n","        \"\"\"\n","        prediction = None\n","        ############################################################################\n","        # TODO: Your code here!\n","        # implement the forward pass here\n","    \n","        ############################################################################\n","        return prediction\n"]},{"cell_type":"code","execution_count":null,"id":"ea04be76","metadata":{"id":"ea04be76"},"outputs":[],"source":["# training configs\n","EPOCHS = 10\n","lr = 1e-4\n","DIM = 16\n","MLP_DIMS = [DIM,DIM]\n","device = \"cuda\"\n","\n","# build model\n","model = DeepFM(field_dims,embed_dim=DIM,mlp_dims=MLP_DIMS,dropout=0.3).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","objective_function = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":null,"id":"751f439b","metadata":{"id":"751f439b"},"outputs":[],"source":["for epoch in range(EPOCHS):\n","    for fields, target in train_loader:\n","        fields, target = fields.to(device), target.to(device)\n","        prediction = model(fields)\n","        loss = objective_function(prediction, target.float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    valid_roc = test(model, valid_loader,device)\n","    print(f\"Validation ROC score:{valid_roc:.4f}\")\n","    \n","test_roc = test(model, test_loader,device)\n","print(f\"Testing ROC score:{test_roc:.4f}\")"]},{"cell_type":"markdown","id":"48badf5a","metadata":{"id":"48badf5a"},"source":["## Practice: Exploring other variants of FM family with `torchfm`\n","[Torchfm](https://github.com/rixwew/pytorch-fm) provides a PyTorch implementation of factorization machine models and common datasets in CTR prediction.<br>\n","Try to play with different models and see which one achieves the best performance!\n","\n","## Available Models\n","\n","| Model | Reference |\n","|-------|-----------|\n","| Logistic Regression | |\n","| Factorization Machine | [S Rendle, Factorization Machines, 2010.](https://www.csie.ntu.edu.tw/~b97053/paper/Rendle2010FM.pdf) |\n","| Field-aware Factorization Machine | [Y Juan, et al. Field-aware Factorization Machines for CTR Prediction, 2015.](https://www.csie.ntu.edu.tw/~cjlin/papers/ffm.pdf) |\n","| Higher-Order Factorization Machines | [ M Blondel, et al. Higher-Order Factorization Machines, 2016.](https://dl.acm.org/doi/10.5555/3157382.3157473) |\n","| Factorization-Supported Neural Network | [W Zhang, et al. Deep Learning over Multi-field Categorical Data - A Case Study on User Response Prediction, 2016.](https://arxiv.org/abs/1601.02376) |\n","| Wide&Deep | [HT Cheng, et al. Wide & Deep Learning for Recommender Systems, 2016.](https://arxiv.org/abs/1606.07792) |\n","| Attentional Factorization Machine | [J Xiao, et al. Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks, 2017.](https://arxiv.org/abs/1708.04617) |\n","| Neural Factorization Machine | [X He and TS Chua, Neural Factorization Machines for Sparse Predictive Analytics, 2017.](https://arxiv.org/abs/1708.05027) |\n","| Neural Collaborative Filtering | [X He, et al. Neural Collaborative Filtering, 2017.](https://arxiv.org/abs/1708.05031) |\n","| Field-aware Neural Factorization Machine | [L Zhang, et al. Field-aware Neural Factorization Machine for Click-Through Rate Prediction, 2019.](https://arxiv.org/abs/1902.09096) |\n","| Product Neural Network | [Y Qu, et al. Product-based Neural Networks for User Response Prediction, 2016.](https://arxiv.org/abs/1611.00144) |\n","| Deep Cross Network | [R Wang, et al. Deep & Cross Network for Ad Click Predictions, 2017.](https://arxiv.org/abs/1708.05123) |\n","| DeepFM | [H Guo, et al. DeepFM: A Factorization-Machine based Neural Network for CTR Prediction, 2017.](https://arxiv.org/abs/1703.04247) |\n","| xDeepFM | [J Lian, et al. xDeepFM: Combining Explicit and Implicit Feature Interactions for Recommender Systems, 2018.](https://arxiv.org/abs/1803.05170) |\n","| AutoInt (Automatic Feature Interaction Model) | [W Song, et al. AutoInt: Automatic Feature Interaction Learning via Self-Attentive Neural Networks, 2018.](https://arxiv.org/abs/1810.11921) |\n","| AFN(AdaptiveFactorizationNetwork Model) | [Cheng W, et al. Adaptive Factorization Network: Learning Adaptive-Order Feature Interactions, AAAI'20.](https://arxiv.org/pdf/1909.03276.pdf) |"]},{"cell_type":"code","execution_count":null,"id":"97506580","metadata":{"id":"97506580"},"outputs":[],"source":["! pip install torchfm"]},{"cell_type":"code","execution_count":null,"id":"a3a6ed50","metadata":{"id":"a3a6ed50"},"outputs":[],"source":["from torchfm.model.afi import AutomaticFeatureInteractionModel\n","from torchfm.model.xdfm import ExtremeDeepFactorizationMachineModel"]},{"cell_type":"code","execution_count":null,"id":"d569be0b","metadata":{"id":"d569be0b"},"outputs":[],"source":["# training configs\n","EPOCHS = 20\n","lr = 1e-4\n","DIM = 16\n","MLP_DIMS = [DIM,DIM]\n","device = \"cuda\"\n","\n","# build model\n","# model = AutomaticFeatureInteractionModel(field_dims,embed_dim=DIM,atten_embed_dim=DIM,num_heads=2,num_layers=2,mlp_dims=MLP_DIMS,dropouts=[0,0]).to(device)\n","model = ExtremeDeepFactorizationMachineModel(field_dims,embed_dim=DIM,mlp_dims=MLP_DIMS,dropout=0.,cross_layer_sizes=MLP_DIMS).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","objective_function = nn.BCEWithLogitsLoss()"]},{"cell_type":"code","execution_count":null,"id":"88aeb706","metadata":{"id":"88aeb706"},"outputs":[],"source":["for epoch in range(EPOCHS):\n","    for fields, target in train_loader:\n","        fields, target = fields.to(device), target.to(device)\n","        prediction = model(fields)\n","        loss = objective_function(prediction, target.float())\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","    valid_roc = test(model, valid_loader,device)\n","    print(f\"Validation ROC score:{valid_roc:.4f}\")\n","    \n","test_roc = test(model, test_loader,device)\n","print(f\"Testing ROC score:{test_roc:.4f}\")"]}],"metadata":{"kernelspec":{"display_name":"pyg","language":"python","name":"pyg"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"3-Factorization-Machine.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}