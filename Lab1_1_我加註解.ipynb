{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab1-1.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhe0/aia_nlp/blob/main/Lab1_1_%E6%88%91%E5%8A%A0%E8%A8%BB%E8%A7%A3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation the CLTS corpus"
      ],
      "metadata": {
        "id": "7p9tO6zypgqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "FBsSjkGMAyPM",
        "outputId": "f712434c-328b-42e6-9891-7c36023322a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Aug 27 05:12:28 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P0    27W /  70W |    970MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "5cTtMSpWXfVD",
        "outputId": "35ac68bf-41a2-4605-e486-d730af9350e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Feb  3 03:42:41 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   47C    P0    27W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lppBOkJMkt7p",
        "outputId": "cfba5a31-3986-4d72-9086-14ed6b0c3471"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change path to our directory\n",
        "%mkdir /content/drive/MyDrive/summarization_lab\n",
        "%cd /content/drive/MyDrive/summarization_lab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JrMIA3xHk2aC",
        "outputId": "602ee82c-dc5a-4428-8c51-8c4674546e39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/summarization_lab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset with gdown\n",
        "!gdown 1VXoRZNfSyegttI1BEBQuklsMBcda2Xmb # test.src\n",
        "!gdown 1SQP90BJvOcSv-68f41Bogz5kSk1W19Kp # test.tgt\n",
        "!gdown 1jPl72PsJODvVuJgVHlDgVG66xlZnSavL # train.src\n",
        "!gdown 1x2I0Q0ElZugOUXatBJiGfTQwAr8CoJIr # train.tgt\n",
        "!gdown 18uHY_DIyPG6XdlzWEmhlJMhfBrNMS-XB # valid.src\n",
        "!gdown 1rCyWRHV-sEL6zhYTbL4Vifowjw5Mqbjd # valid.tgt"
      ],
      "metadata": {
        "id": "gtyPMWtHlA4k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53c11c92-48f0-47b2-dffd-c922463bb764"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1VXoRZNfSyegttI1BEBQuklsMBcda2Xmb\n",
            "To: /content/drive/MyDrive/summarization_lab/test.src\n",
            "100% 101M/101M [00:00<00:00, 260MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SQP90BJvOcSv-68f41Bogz5kSk1W19Kp\n",
            "To: /content/drive/MyDrive/summarization_lab/test.tgt\n",
            "100% 4.28M/4.28M [00:00<00:00, 132MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1jPl72PsJODvVuJgVHlDgVG66xlZnSavL\n",
            "To: /content/drive/MyDrive/summarization_lab/train.src\n",
            "100% 904M/904M [00:03<00:00, 244MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1x2I0Q0ElZugOUXatBJiGfTQwAr8CoJIr\n",
            "To: /content/drive/MyDrive/summarization_lab/train.tgt\n",
            "100% 38.0M/38.0M [00:00<00:00, 93.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=18uHY_DIyPG6XdlzWEmhlJMhfBrNMS-XB\n",
            "To: /content/drive/MyDrive/summarization_lab/valid.src\n",
            "100% 125M/125M [00:00<00:00, 252MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rCyWRHV-sEL6zhYTbL4Vifowjw5Mqbjd\n",
            "To: /content/drive/MyDrive/summarization_lab/valid.tgt\n",
            "100% 5.21M/5.21M [00:00<00:00, 129MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_CLTS(data_type, islimit1千=True):\n",
        "    data_dir = \"/content/drive/MyDrive/summarization_lab\"\n",
        "    with open(os.path.join(data_dir, \"%s.src\" % data_type)) as fsrc, open(os.path.join(data_dir, \"%s.tgt\" % data_type)) as ftgt:\n",
        "        cnt=0\n",
        "        if islimit1千==True:\n",
        "          stopped=1000\n",
        "        else:\n",
        "          stopped=-1\n",
        "        for src, tgt in zip(fsrc, ftgt):\n",
        "            src = src.replace(\" \", \"\")\n",
        "            tgt = tgt.replace(\" \", \"\")\n",
        "            cnt+=1\n",
        "            if cnt == stopped:\n",
        "              break\n",
        "            yield (src, tgt)\n",
        "\n",
        "# for src, tgt in load_CLTS(\"valid\"):\n",
        "#     print(src)\n",
        "#     print(tgt)\n",
        "#     break\n",
        "\n",
        "# cnt=0\n",
        "# for src, tgt in load_CLTS(\"test\",islimit兩千=True):\n",
        "#   cnt+=1\n",
        "# cnt"
      ],
      "metadata": {
        "id": "IPl_81Yx5d7X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Lead K Model"
      ],
      "metadata": {
        "id": "U_CMLxKV6Ylf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LeadKSummarizer:\n",
        "    def __init__(self, k=3):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "\n",
        "    def perform(self, text=\"\"):\n",
        "        sents = text.split(\"。\")\n",
        "        return \"。\".join(sents[:self.k])\n",
        "\n",
        "model = LeadKSummarizer()\n",
        "\n",
        "for src, tgt in load_CLTS(\"valid\"):\n",
        "    print(src)\n",
        "    print(tgt)\n",
        "    print(model.perform(src))\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_A0FydKo6Xst",
        "outputId": "9fcdfa31-e9a7-4538-efdc-ec01610c6448"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "（图片来源于网络）文|易不二来源|螳螂财经（ID:TanglangFin）拿了“时尚电商第一股”的名头上市的蘑菇街，才过去大半年，业绩跌落速度却比时尚过气速度还快。截止8月15日收盘，蘑菇街总市值已经跌到只剩2.55亿，与2016年合并美丽说时候的30亿估值相比，已经走到了大失血的地步，与2018年底上市时15亿美元的市值比也相去甚远。成立于2011年的蘑菇街，从最初的女性社区到垂直电商，现在又回到社交电商，几经改变，却一再错失良机。作为电商平台，品类和价格都不具备拼多多的优势；作为内容平台，却又没有小红书的优质内容与影响力。如今，财报上略微好看的直播业务，似乎成了蘑菇街的救命稻草。一度被冠之以电商“第四极”光环的蘑菇街，走到了如今的地步，究竟是怎么了？错失社区“开鱼刀”，蘑菇街进退失据蘑菇街从女性社区转型做垂直电商，是被阿里逼的。最初定位导购平台的蘑菇街和美丽说两家平台，在鼎盛期，一度能每天向淘宝导入300-500万的UV，电商导购转化率高达10%。虽说大树底下好乘凉，但过度依赖淘宝，并不是长久之计。“我要是马云，早把蘑菇街封了。”连蘑菇街联合创始人魏一搏也这样调侃自己。2013年，淘宝取消了蘑菇街和美丽说的淘宝客联盟资格，屏蔽了所有外链，两个平台的商业模式彻底崩塌。改变关头，蘑菇街直接关闭原有的社区，选择了正处于发展高位的垂直电商，此时，不管是唯品会还是聚美优品，都展示着垂直电商无限的生命力。蘑菇街似乎看到了自己无限的未来。然而，垂直电商的生命力几年之后变迅速萎谢，而被蘑菇街关闭的社区，却成为了它错失的“开鱼刀”。“开鱼刀”的典故来自荷兰。17世纪之前，荷兰只是欧洲一个非常普通的国家。森林不广袤，矿产不丰富，倒是鲱鱼数量多到让人震惊。这种体型很小的鱼处理麻烦，不易保存，两三天就臭了。虽然数量多，但荷兰人却无法将其利用起来致富。后来，有一位叫叫威廉·布克尔松的渔民发明了一把能将鲱鱼一刀就轻松处理好的开鱼刀，正是这把小小的开鱼刀，极大地提升了荷兰人处理鲱鱼的效率，处理好的鲱鱼化为盐腌保存，远销世界各国。荷兰经济也由此开始迅速腾飞，成为一代海上霸主。对于企业来说，开鱼刀，就是让自己立足于市场的差异化业务破局点。而被蘑菇街毫不犹豫关闭的UGC社区，却成了小红书手里的“开鱼刀”。2013年，小红书以分享海外旅行购物消费体验等进入年轻人的世界。截至2019年1月，小红书用户数已超过2亿，其中70%用户是90后。如今，社区已经成为小红书的壁垒，是其他平台无法复制的地方。眼看着自己曾经的模式被小红书实践成功，蘑菇街又急又眼红，也打算花大力气重建社区。2019年2月27日，蘑菇街发内部信称，要着力于建立达人社区，集中精力提升社区内容质量，同时，持续完善品牌库。这是要复制小红书？蘑菇街是不是只看到了小红书的成功，却没看到成功背后的焦虑。还是说，蘑菇街已经不知道自己要走什么路了，不如就走一条老路？但是，要知道，小红书自己至今也没有找到一条社区与电商完美结合的变现路径，并且还砍掉了自有品牌“有光”业务，降低电商业务的占比，将重心依然放在社区上。如今，由于内容的失控，小红书还处于下架整改阶段。况且，在“螳螂财经”看来，蘑菇街的社区还有多方面的不足。首先，品类受限，天花板低。蘑菇街定位于为年轻女性消费者提供服饰、鞋包等的穿戴搭配，单一的品类很难成为构建内容护城河。其次，价格低廉，品控不稳。蘑菇街的产品客单价大多在100元左右，价格的美丽，带来的是质量的不美丽，让多数用户种草不拔草。而随着新中产的崛起，品质成为了更关注的问题。CBNData发布的《新时代“自我关爱型”年轻女性金钱观与消费趋势洞察报告》显示，购买贵价和奢价产品的女性消费者中，90后比重逐年上升。还在追求低价的蘑菇街，后期会遇到很大的用户留存难点。最后，用户参与度低。蘑菇街社区的多数帖子都来自于精选达人，用户的内容很少，参与感低，这势必会导致用户的粘性不高。KOL的最大价值在于以自己身影响力和分享的内容吸引到粉丝留存，继而一起参与到内容分享，增加用户的活跃性。目前来说，也许蘑菇街还在以达人内容吸引用户留存吧。如今，UGC社区已然成为各大电商平台的必争之地。此前封杀蘑菇街的阿里，于2018年6月，以3亿美元领投了小红书的D轮融资，让小红书的估值达30亿美元大关。错失先机的蘑菇街，如今在UGC社区上进退失据。进，超越不了小红书；退，已无路可走。起了个大早，还一度手握“开鱼刀”，却赶了个集，实在令人唏嘘。直播这把新的“开鱼刀”，蘑菇街能用好吗？错失社区这把“开鱼刀”后，蘑菇街找到了新的“开鱼刀”。2015年直播火了。2016年3月蘑菇街迅速跟进，正式上线直播功能，开放直播间直接下单功能。之后，APP上留出重要资源位给直播入口，微信上线“蘑菇街购物台”小程序，致力于构建“社交+直播+电商”的新模式。目前来看，蘑菇街似乎用好了这把“开鱼刀”，带来了还不错的成绩。根据蘑菇街招股书，2017财年，蘑菇街直播GMV仅有2亿元，相当于总交易额大约1.4%；2018财年直播贡献GMV为17亿元，占总交易额11.8%；与2017财年相比，无论是绝对值，还是占比，都有较大提升。截至2018年9月30日止的最近6个月内，视频直播GMV继续上扬至14亿元，占总交易额提高到18%。“我们通过丰富内容，提升用户对于直播业务的参与度，促成复购率的提升，继续扩展、优化和提升了时尚生态系统的供应链。”蘑菇街CEO陈琪表示。蘑菇街最新的财报肯定了这一点，本季度蘑菇街的直播业务GMV同比增长177.8%，2018年直播的移动月活用户数增长43.6%，活跃用户平均每天在蘑菇街平台上花费的时间达到54分钟以上。然而，表现亮眼的直播业务背后，却是流量见顶的危机。2018年第四季度，蘑菇街的活跃买家数量为3450万，比去年同期的3390万仅增长1.8%。与此同时，阿里巴巴最新财报显示，淘宝移动月度活跃用户达到6.99亿，较2018年9月增加3300万，巨大的流量给淘宝直播带来的月活用户同比增长超过100%。对比之下，在蘑菇街用户增量很小存量有限的情况下，可以预见，直播业务将会很快摸到天花板。7月底，蘑菇街发布“2019蘑菇街直播双百计划”，计划面向全网招募优质红人主播、机构以及供应链，并将在2019年度内孵化100个销售额破千万的主播。意在以直播为切入点，深入供应链、链接品牌商，形成到用户的闭环，构建自己的时尚生态。蘑菇街死磕直播业务，电商大佬们也没闲着。同样是7月底，京东计划至少投入10亿资源，孵化不超过5名超级网红成为“京品推荐官”；8月1日，网易考拉宣布正式上线直播，初期将以美妆、母婴等平台核心品类为主，通过美妆达人和辣妈KOL直播进行知识类导购；8月3日，微博超级红人节上，微博高级副总裁曹增辉宣布将推出电商服务平台，同时微博电商直播将与淘宝实现打通；8月6日，苏宁易购宣布正式打通快手小店，消费者可通过快手小店跳转至苏宁易购完成购物。淘宝直播有多品类带来的丰富内容；网易考拉有直击海外原产地和产业带的特色IP；苏宁背靠快手的巨大流量......而围绕“人货场”展开直播的蘑菇街，人方面，没有比得过“淘宝一姐”薇娅的高质量主播；货方面，定位服饰和美妆，品类有天然的基因缺陷；场方面，怎样为用户增加更多的场景设计，让直播的内容更丰富好玩，也是之后该解决的难题。各大巨头的进击下，蘑菇街“亮眼”的直播，也成了夹缝求生的业务。但“螳螂财经”认为，蘑菇街也还是有机会的，具体表现在两个方面。第一个，微信赋能蘑菇街小程序。“微信的覆盖人群是最广的，这一渠道不仅拥有支付这一核心基础设施，同时又具备社交功能，再加上小程序这个运营环境，已经开始形成一个生态。”蘑菇街的小程序在上线半年时间获取了9000万客户，直播购物台去年的双十一销量增长则同期上涨将近30倍。第二个，打造时尚生态。“总的来说，我们的目标是形成一横一纵的逻辑。一横即尽可能多地覆盖所有流行的时尚穿搭资讯，一纵则是要打通相关产品在市场中的流通环节，全面满足消费者对时尚的追求与需求，同时扶持更多具有潜力的迷你品牌。”基于时尚这个点，纵横延伸，打造从供应链、KOL、到用户的生态闭环，或将能成为蘑菇街的竞争壁垒。不管是通过微信小程序逐渐把微信用户引流到蘑菇街APP上，还是深入供应链、链接品牌商纵横延伸发展自己的时尚生态，蘑菇街都必须加快步伐。资本的耐心是有限的，而有了阿里扶持的小红书等对手的成长也是飞速的，留给蘑菇街的时间是不多的。未来，随着5G的到来，直播还有很大的想象空间。就看蘑菇街能不能用好这把“开鱼刀”，拯救自己于“扑街”。结语雕爷的文章里曾说“千万别死守某把开鱼刀，一个方法不灵赶紧第二个，第二个不给力咱换第三个，快速切换到能令你战略可以落地的那把最见效率的开鱼刀。”这种切换，就是时刻保持自身跟对手之间的差异化竞争力。对于一直在社区与电商间摇摆的蘑菇街，前期没有找定自己的业务方向，也一再错失撕开差异化竞争的“开鱼刀”，在严重的同质化中，走到市值大失血的地步。希望手握直播“开鱼刀”的蘑菇街，在市场的变化中，拿捏好切换的尺度，别让错失再上演。*此内容为【螳螂财经】原创，未经授权，任何人不得以任何方式使用，包括转载、摘编、复制或建立镜像。【完】螳螂财经（微信ID:TanglangFin）：泛财经新媒体，《财富生活》等多家杂志特约撰稿人。微信十万+曝文《京东走向“四分五裂”》创作者；重点关注：新金融、新零售、上市公司等财经金融等领域。\n",
            "\n",
            "对于一直在社区与电商间摇摆的蘑菇街，前期没有找定自己的业务方向，也一再错失撕开差异化竞争的“开鱼刀”，在严重的同质化中，走到市值大失血的地步。\n",
            "\n",
            "（图片来源于网络）文|易不二来源|螳螂财经（ID:TanglangFin）拿了“时尚电商第一股”的名头上市的蘑菇街，才过去大半年，业绩跌落速度却比时尚过气速度还快。截止8月15日收盘，蘑菇街总市值已经跌到只剩2.55亿，与2016年合并美丽说时候的30亿估值相比，已经走到了大失血的地步，与2018年底上市时15亿美元的市值比也相去甚远。成立于2011年的蘑菇街，从最初的女性社区到垂直电商，现在又回到社交电商，几经改变，却一再错失良机\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation of the Generated Outcomes"
      ],
      "metadata": {
        "id": "wkKschJd7Cim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cost 2mins\n",
        "!pip install rouge_metric"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6ImjBRKDBlC",
        "outputId": "7b30c5e3-bd56-40ad-ed66-ff309c8edf1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting rouge_metric\n",
            "  Downloading rouge_metric-1.0.1-py3-none-any.whl (151 kB)\n",
            "\u001b[K     |████████████████████████████████| 151 kB 15.4 MB/s \n",
            "\u001b[?25hInstalling collected packages: rouge-metric\n",
            "Successfully installed rouge-metric-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_metric import PyRouge\n",
        "rouge = PyRouge()\n",
        "\n",
        "for src, tgt in load_CLTS(\"valid\"): #這邊取valid 只是因為train數量很多\n",
        "    print(src)\n",
        "    print(tgt)\n",
        "    result = model.perform(src)\n",
        "    print(result)\n",
        "    print(rouge.evaluate([\" \".join(result)], [[\" \".join(tgt)]]))\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF_yFoV67IwG",
        "outputId": "ce34f60a-7c95-4237-e91f-2b54c28e567a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "（图片来源于网络）文|易不二来源|螳螂财经（ID:TanglangFin）拿了“时尚电商第一股”的名头上市的蘑菇街，才过去大半年，业绩跌落速度却比时尚过气速度还快。截止8月15日收盘，蘑菇街总市值已经跌到只剩2.55亿，与2016年合并美丽说时候的30亿估值相比，已经走到了大失血的地步，与2018年底上市时15亿美元的市值比也相去甚远。成立于2011年的蘑菇街，从最初的女性社区到垂直电商，现在又回到社交电商，几经改变，却一再错失良机。作为电商平台，品类和价格都不具备拼多多的优势；作为内容平台，却又没有小红书的优质内容与影响力。如今，财报上略微好看的直播业务，似乎成了蘑菇街的救命稻草。一度被冠之以电商“第四极”光环的蘑菇街，走到了如今的地步，究竟是怎么了？错失社区“开鱼刀”，蘑菇街进退失据蘑菇街从女性社区转型做垂直电商，是被阿里逼的。最初定位导购平台的蘑菇街和美丽说两家平台，在鼎盛期，一度能每天向淘宝导入300-500万的UV，电商导购转化率高达10%。虽说大树底下好乘凉，但过度依赖淘宝，并不是长久之计。“我要是马云，早把蘑菇街封了。”连蘑菇街联合创始人魏一搏也这样调侃自己。2013年，淘宝取消了蘑菇街和美丽说的淘宝客联盟资格，屏蔽了所有外链，两个平台的商业模式彻底崩塌。改变关头，蘑菇街直接关闭原有的社区，选择了正处于发展高位的垂直电商，此时，不管是唯品会还是聚美优品，都展示着垂直电商无限的生命力。蘑菇街似乎看到了自己无限的未来。然而，垂直电商的生命力几年之后变迅速萎谢，而被蘑菇街关闭的社区，却成为了它错失的“开鱼刀”。“开鱼刀”的典故来自荷兰。17世纪之前，荷兰只是欧洲一个非常普通的国家。森林不广袤，矿产不丰富，倒是鲱鱼数量多到让人震惊。这种体型很小的鱼处理麻烦，不易保存，两三天就臭了。虽然数量多，但荷兰人却无法将其利用起来致富。后来，有一位叫叫威廉·布克尔松的渔民发明了一把能将鲱鱼一刀就轻松处理好的开鱼刀，正是这把小小的开鱼刀，极大地提升了荷兰人处理鲱鱼的效率，处理好的鲱鱼化为盐腌保存，远销世界各国。荷兰经济也由此开始迅速腾飞，成为一代海上霸主。对于企业来说，开鱼刀，就是让自己立足于市场的差异化业务破局点。而被蘑菇街毫不犹豫关闭的UGC社区，却成了小红书手里的“开鱼刀”。2013年，小红书以分享海外旅行购物消费体验等进入年轻人的世界。截至2019年1月，小红书用户数已超过2亿，其中70%用户是90后。如今，社区已经成为小红书的壁垒，是其他平台无法复制的地方。眼看着自己曾经的模式被小红书实践成功，蘑菇街又急又眼红，也打算花大力气重建社区。2019年2月27日，蘑菇街发内部信称，要着力于建立达人社区，集中精力提升社区内容质量，同时，持续完善品牌库。这是要复制小红书？蘑菇街是不是只看到了小红书的成功，却没看到成功背后的焦虑。还是说，蘑菇街已经不知道自己要走什么路了，不如就走一条老路？但是，要知道，小红书自己至今也没有找到一条社区与电商完美结合的变现路径，并且还砍掉了自有品牌“有光”业务，降低电商业务的占比，将重心依然放在社区上。如今，由于内容的失控，小红书还处于下架整改阶段。况且，在“螳螂财经”看来，蘑菇街的社区还有多方面的不足。首先，品类受限，天花板低。蘑菇街定位于为年轻女性消费者提供服饰、鞋包等的穿戴搭配，单一的品类很难成为构建内容护城河。其次，价格低廉，品控不稳。蘑菇街的产品客单价大多在100元左右，价格的美丽，带来的是质量的不美丽，让多数用户种草不拔草。而随着新中产的崛起，品质成为了更关注的问题。CBNData发布的《新时代“自我关爱型”年轻女性金钱观与消费趋势洞察报告》显示，购买贵价和奢价产品的女性消费者中，90后比重逐年上升。还在追求低价的蘑菇街，后期会遇到很大的用户留存难点。最后，用户参与度低。蘑菇街社区的多数帖子都来自于精选达人，用户的内容很少，参与感低，这势必会导致用户的粘性不高。KOL的最大价值在于以自己身影响力和分享的内容吸引到粉丝留存，继而一起参与到内容分享，增加用户的活跃性。目前来说，也许蘑菇街还在以达人内容吸引用户留存吧。如今，UGC社区已然成为各大电商平台的必争之地。此前封杀蘑菇街的阿里，于2018年6月，以3亿美元领投了小红书的D轮融资，让小红书的估值达30亿美元大关。错失先机的蘑菇街，如今在UGC社区上进退失据。进，超越不了小红书；退，已无路可走。起了个大早，还一度手握“开鱼刀”，却赶了个集，实在令人唏嘘。直播这把新的“开鱼刀”，蘑菇街能用好吗？错失社区这把“开鱼刀”后，蘑菇街找到了新的“开鱼刀”。2015年直播火了。2016年3月蘑菇街迅速跟进，正式上线直播功能，开放直播间直接下单功能。之后，APP上留出重要资源位给直播入口，微信上线“蘑菇街购物台”小程序，致力于构建“社交+直播+电商”的新模式。目前来看，蘑菇街似乎用好了这把“开鱼刀”，带来了还不错的成绩。根据蘑菇街招股书，2017财年，蘑菇街直播GMV仅有2亿元，相当于总交易额大约1.4%；2018财年直播贡献GMV为17亿元，占总交易额11.8%；与2017财年相比，无论是绝对值，还是占比，都有较大提升。截至2018年9月30日止的最近6个月内，视频直播GMV继续上扬至14亿元，占总交易额提高到18%。“我们通过丰富内容，提升用户对于直播业务的参与度，促成复购率的提升，继续扩展、优化和提升了时尚生态系统的供应链。”蘑菇街CEO陈琪表示。蘑菇街最新的财报肯定了这一点，本季度蘑菇街的直播业务GMV同比增长177.8%，2018年直播的移动月活用户数增长43.6%，活跃用户平均每天在蘑菇街平台上花费的时间达到54分钟以上。然而，表现亮眼的直播业务背后，却是流量见顶的危机。2018年第四季度，蘑菇街的活跃买家数量为3450万，比去年同期的3390万仅增长1.8%。与此同时，阿里巴巴最新财报显示，淘宝移动月度活跃用户达到6.99亿，较2018年9月增加3300万，巨大的流量给淘宝直播带来的月活用户同比增长超过100%。对比之下，在蘑菇街用户增量很小存量有限的情况下，可以预见，直播业务将会很快摸到天花板。7月底，蘑菇街发布“2019蘑菇街直播双百计划”，计划面向全网招募优质红人主播、机构以及供应链，并将在2019年度内孵化100个销售额破千万的主播。意在以直播为切入点，深入供应链、链接品牌商，形成到用户的闭环，构建自己的时尚生态。蘑菇街死磕直播业务，电商大佬们也没闲着。同样是7月底，京东计划至少投入10亿资源，孵化不超过5名超级网红成为“京品推荐官”；8月1日，网易考拉宣布正式上线直播，初期将以美妆、母婴等平台核心品类为主，通过美妆达人和辣妈KOL直播进行知识类导购；8月3日，微博超级红人节上，微博高级副总裁曹增辉宣布将推出电商服务平台，同时微博电商直播将与淘宝实现打通；8月6日，苏宁易购宣布正式打通快手小店，消费者可通过快手小店跳转至苏宁易购完成购物。淘宝直播有多品类带来的丰富内容；网易考拉有直击海外原产地和产业带的特色IP；苏宁背靠快手的巨大流量......而围绕“人货场”展开直播的蘑菇街，人方面，没有比得过“淘宝一姐”薇娅的高质量主播；货方面，定位服饰和美妆，品类有天然的基因缺陷；场方面，怎样为用户增加更多的场景设计，让直播的内容更丰富好玩，也是之后该解决的难题。各大巨头的进击下，蘑菇街“亮眼”的直播，也成了夹缝求生的业务。但“螳螂财经”认为，蘑菇街也还是有机会的，具体表现在两个方面。第一个，微信赋能蘑菇街小程序。“微信的覆盖人群是最广的，这一渠道不仅拥有支付这一核心基础设施，同时又具备社交功能，再加上小程序这个运营环境，已经开始形成一个生态。”蘑菇街的小程序在上线半年时间获取了9000万客户，直播购物台去年的双十一销量增长则同期上涨将近30倍。第二个，打造时尚生态。“总的来说，我们的目标是形成一横一纵的逻辑。一横即尽可能多地覆盖所有流行的时尚穿搭资讯，一纵则是要打通相关产品在市场中的流通环节，全面满足消费者对时尚的追求与需求，同时扶持更多具有潜力的迷你品牌。”基于时尚这个点，纵横延伸，打造从供应链、KOL、到用户的生态闭环，或将能成为蘑菇街的竞争壁垒。不管是通过微信小程序逐渐把微信用户引流到蘑菇街APP上，还是深入供应链、链接品牌商纵横延伸发展自己的时尚生态，蘑菇街都必须加快步伐。资本的耐心是有限的，而有了阿里扶持的小红书等对手的成长也是飞速的，留给蘑菇街的时间是不多的。未来，随着5G的到来，直播还有很大的想象空间。就看蘑菇街能不能用好这把“开鱼刀”，拯救自己于“扑街”。结语雕爷的文章里曾说“千万别死守某把开鱼刀，一个方法不灵赶紧第二个，第二个不给力咱换第三个，快速切换到能令你战略可以落地的那把最见效率的开鱼刀。”这种切换，就是时刻保持自身跟对手之间的差异化竞争力。对于一直在社区与电商间摇摆的蘑菇街，前期没有找定自己的业务方向，也一再错失撕开差异化竞争的“开鱼刀”，在严重的同质化中，走到市值大失血的地步。希望手握直播“开鱼刀”的蘑菇街，在市场的变化中，拿捏好切换的尺度，别让错失再上演。*此内容为【螳螂财经】原创，未经授权，任何人不得以任何方式使用，包括转载、摘编、复制或建立镜像。【完】螳螂财经（微信ID:TanglangFin）：泛财经新媒体，《财富生活》等多家杂志特约撰稿人。微信十万+曝文《京东走向“四分五裂”》创作者；重点关注：新金融、新零售、上市公司等财经金融等领域。\n",
            "\n",
            "对于一直在社区与电商间摇摆的蘑菇街，前期没有找定自己的业务方向，也一再错失撕开差异化竞争的“开鱼刀”，在严重的同质化中，走到市值大失血的地步。\n",
            "\n",
            "（图片来源于网络）文|易不二来源|螳螂财经（ID:TanglangFin）拿了“时尚电商第一股”的名头上市的蘑菇街，才过去大半年，业绩跌落速度却比时尚过气速度还快。截止8月15日收盘，蘑菇街总市值已经跌到只剩2.55亿，与2016年合并美丽说时候的30亿估值相比，已经走到了大失血的地步，与2018年底上市时15亿美元的市值比也相去甚远。成立于2011年的蘑菇街，从最初的女性社区到垂直电商，现在又回到社交电商，几经改变，却一再错失良机\n",
            "{'rouge-1': {'r': 0.5492957746478874, 'p': 0.17889908256880735, 'f': 0.2698961937716263}, 'rouge-2': {'r': 0.22857142857142856, 'p': 0.07373271889400922, 'f': 0.11149825783972125}, 'rouge-l': {'r': 0.30985915492957744, 'p': 0.10091743119266056, 'f': 0.1522491349480969}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation on the Entire Test Set"
      ],
      "metadata": {
        "id": "G3U9qGfM8kR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "references = []\n",
        "for src, tgt in load_CLTS(\"test\"):\n",
        "    results.append(\" \".join(model.perform(src)))\n",
        "    references.append([\" \".join(tgt)])\n",
        "print(rouge.evaluate(results, references))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PKug2kjM81cG",
        "outputId": "fc7862ce-7067-4e05-fd06-c4ff8a6cb5d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.7255529855170463, 'p': 0.19565912533375748, 'f': 0.3082049418530782}, 'rouge-2': {'r': 0.4994347756690937, 'p': 0.1409670918443322, 'f': 0.2198740243071592}, 'rouge-l': {'r': 0.5735131732270399, 'p': 0.15884087330009702, 'f': 0.24877949051140333}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "\n",
        "Rouge-2 和 Rouge-L 有沒有可能比 Rouge-1 高？為什麼？\n",
        "ans:沒有，最多持平"
      ],
      "metadata": {
        "id": "blyVqeZb9rmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding the Best k with the Validation Set"
      ],
      "metadata": {
        "id": "z8vslv7q-KXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# for src, tgt in load_CLTS('test'):\n",
        "#   break\n",
        "# src"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "sMfsA3Y1gsa3",
        "outputId": "7f7db589-5112-49e6-ee55-b1a4cc4a1219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'中新网3月4日电国台办发言人朱凤莲3月4日表示，由于民进党当局一再阻挠，1148名急需返乡的滞鄂台胞迄今无法回家。苏贞昌日前又公开散布“苏式谎言”，继续罔顾事实、颠倒黑白，谎称“卡关就卡在大陆”，“真不知人间还有羞耻二字。”朱凤莲说，疫情发生以来，大陆方面一方面全力照顾在大陆台胞的生活和疫情防控需要，另一方面充分考虑滞鄂台胞的实际需求和回家心愿，积极安排东航于2月3日运送首批247名台胞返回台湾，并于2月5日和此后多次提出尽快运送其他提出返乡要求台胞的合理安排，包括提出由两岸航空公司共同执飞临时航班的运送安排，以满足滞鄂台胞急切回家的愿望。但民进党当局却一而再、再而三变换借口，不断设置障碍，一再拖延阻挠。“2月15日，我办发言人已详细披露大陆方面持续做出运送台胞安排和为实现运送不懈努力的全过程和细节，具体情况清清楚楚，事实真相一目了然。”朱凤莲指出，民进党当局不断以各种借口阻止东航后续运送，有目共睹。苏贞昌自己就曾公开说过，不能让在湖北的台胞回去，是因为岛内防疫安置能量不足。更有甚者，民进党当局竟然将期待返乡就业、学习团聚等1148名台胞列入所谓“注记管制名单”，全面封堵了滞鄂台胞回家之路。事实反复证明，民进党当局根本就不想让在湖北的台胞回家，滞鄂台胞返乡之路受阻，“卡关”就卡在民进党当局的这些政客手中。朱凤莲强调，苏贞昌企图以自相矛盾的谎言转移视线、推卸责任，未免低估了广大台胞的智商。“我们奉劝他要有起码的道德底线，停止信口雌黄，停止造谣生事。我们质问他，敢不敢讲立即同意这1148名台胞返乡？”（原题为《国台办：奉劝苏贞昌停止造谣说谎》）(本文来自澎湃新闻，更多原创资讯请下载“澎湃新闻”APP)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#    def perform(self, text):\n",
        "        # sents = [s for s in text.split(\"。\") if len(set(s)) > 1]\n",
        "        # return \"。\".join(self.text_rank(sents)[:self.k])\n",
        "\n",
        "        # ranked = []\n",
        "        # for sent_id, w in sorted(list(text_rank.items()), key=lambda x: -x[1]):\n",
        "        #     ranked.append(sents[sent_id])\n",
        "        # return ranked\n",
        "# loop 引數model所輸出的每一句的r1、r2看看哪句適合做summarization\n",
        "def evaluate(model, datatype=\"test\"):\n",
        "    results = []\n",
        "    references = []\n",
        "    for src, tgt in load_CLTS(datatype):\n",
        "        results.append(\" \".join(model.perform(src))) #perform執行 句點接回list 並執行text simi rank\n",
        "        references.append([\" \".join(tgt)])\n",
        "    return rouge.evaluate(results, references)\n",
        "\n",
        "best_k = 1\n",
        "best_r1 = 0\n",
        "for k in range(1, 10):\n",
        "    model = LeadKSummarizer(k)\n",
        "    results = evaluate(model, \"valid\")\n",
        "    print(k)\n",
        "    print(results)\n",
        "    if results['rouge-1']['f'] > best_r1:\n",
        "        best_r1 = results['rouge-1']['f']\n",
        "        best_k = k"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_wCL0ae-Pc_",
        "outputId": "34127bd3-cf54-44a7-9248-5b3c7b6d1504"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "{'rouge-1': {'r': 0.4226295144633724, 'p': 0.47383237341303774, 'f': 0.4467686772205257}, 'rouge-2': {'r': 0.3344916505337728, 'p': 0.35785934124803864, 'f': 0.34578115185457214}, 'rouge-l': {'r': 0.3757801060098189, 'p': 0.4169482198695172, 'f': 0.3952951878927092}}\n",
            "2\n",
            "{'rouge-1': {'r': 0.6371564954891635, 'p': 0.38432539217057016, 'f': 0.4794513205983921}, 'rouge-2': {'r': 0.5081936214176388, 'p': 0.29818333153926957, 'f': 0.37584126516923316}, 'rouge-l': {'r': 0.5568436598633802, 'p': 0.3318985919427807, 'f': 0.41590377022207736}}\n",
            "3\n",
            "{'rouge-1': {'r': 0.7472241713596787, 'p': 0.30944339447752384, 'f': 0.4376467897696745}, 'rouge-2': {'r': 0.6131550047607034, 'p': 0.2483789055823492, 'f': 0.35354329575759047}, 'rouge-l': {'r': 0.6617102592026719, 'p': 0.2713900013012968, 'f': 0.3849137240817023}}\n",
            "4\n",
            "{'rouge-1': {'r': 0.8045186980779521, 'p': 0.25483338038538095, 'f': 0.3870634203348975}, 'rouge-2': {'r': 0.6788500637693086, 'p': 0.21076440245192968, 'f': 0.321661648899515}, 'rouge-l': {'r': 0.726000677049701, 'p': 0.22846587827277795, 'f': 0.3475582908250803}}\n",
            "5\n",
            "{'rouge-1': {'r': 0.844617492053266, 'p': 0.21953262873807422, 'f': 0.3484867307457095}, 'rouge-2': {'r': 0.7249017871612506, 'p': 0.1854835583079342, 'f': 0.2953856047345629}, 'rouge-l': {'r': 0.7690420749080298, 'p': 0.19913145560479306, 'f': 0.31634921420884277}}\n",
            "6\n",
            "{'rouge-1': {'r': 0.8687020388296145, 'p': 0.19399832377134876, 'f': 0.31716699329475806}, 'rouge-2': {'r': 0.7525998491769028, 'p': 0.16590176205089913, 'f': 0.2718724487173982}, 'rouge-l': {'r': 0.7962148889754092, 'p': 0.17754621417455815, 'f': 0.29034829744114077}}\n",
            "7\n",
            "{'rouge-1': {'r': 0.8886633813675558, 'p': 0.1758699798085309, 'f': 0.2936295218874605}, 'rouge-2': {'r': 0.7771859314823922, 'p': 0.1525106082445321, 'f': 0.25498449023869213}, 'rouge-l': {'r': 0.819034291726713, 'p': 0.1622619992457379, 'f': 0.27086241505038844}}\n",
            "8\n",
            "{'rouge-1': {'r': 0.9030481801733173, 'p': 0.16207685470864328, 'f': 0.27482821997339635}, 'rouge-2': {'r': 0.7961089383498011, 'p': 0.14230215746584896, 'f': 0.24144646202537776}, 'rouge-l': {'r': 0.8356824947832585, 'p': 0.15052370770965603, 'f': 0.25509883686568097}}\n",
            "9\n",
            "{'rouge-1': {'r': 0.9159090199449863, 'p': 0.15154986617002822, 'f': 0.26006788870672926}, 'rouge-2': {'r': 0.8126223719823005, 'p': 0.13425061434127394, 'f': 0.23043228446018013}, 'rouge-l': {'r': 0.8508762416603788, 'p': 0.14154165116981035, 'f': 0.24270910280006391}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TextRank Summarization Model\n",
        "\n",
        "$Similarity(S_i, S_j) = \\frac{|S_i \\cap S_j | }{ \\log(|S_i|) + \\log(|S_j|) }$"
      ],
      "metadata": {
        "id": "th095Bld9cVW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict \n",
        "\n",
        "class TextRankSummarizer:\n",
        "    \"\"\"Original paper can be found here https://aclanthology.org/W04-3252.pdf     \n",
        "    \"\"\"\n",
        "    def __init__(self, k=3, damping=0.85, num_iters=100):\n",
        "        self.damping = damping\n",
        "        self.num_iters = num_iters\n",
        "        self.k = k\n",
        "    \n",
        "    def similarity(self, s1, s2):\n",
        "        s1 = set(list(s1))\n",
        "        s2 = set(list(s2))\n",
        "        from math import log\n",
        "        try:\n",
        "          #是因為set才能做 &intersection交集處理\n",
        "          #優點這邊不限句子長度、任意字跟任意句子可以計算\n",
        "          #相較於向量間的cosine，少了一文多義(bert)\n",
        "            return len(s1 & s2) / (log(len(s1)) + log(len(s2)))\n",
        "        except:\n",
        "            print(s1)\n",
        "            print(s2)\n",
        "            print((log(len(s1)) + log(len(s2))))\n",
        "            return 0\n",
        " \n",
        "    def text_rank(self, sents):\n",
        "        graph = defaultdict(dict) # defaultdict(list) # default值以一個list()方法產生\n",
        "        num_sents = len(sents)\n",
        "        text_rank = {}\n",
        "\n",
        "        for i in range(num_sents):\n",
        "            text_rank[i] = 1.0 / num_sents\n",
        "            for j in range(i + 1, num_sents):\n",
        "                graph[i][j] = self.similarity(sents[i], sents[j])\n",
        "                graph[j][i] = graph[i][j]\n",
        "\n",
        "        nw = defaultdict(dict)\n",
        "        for i in range(num_sents):\n",
        "            s = 0\n",
        "            for j in range(num_sents):\n",
        "                if i != j:\n",
        "                    s += graph[i][j]\n",
        "            for j in range(num_sents):\n",
        "                if i != j:\n",
        "                    if s == 0:\n",
        "                        nw[i][j] = 0 \n",
        "                    else:\n",
        "                        nw[i][j] = graph[i][j] / s\n",
        "\n",
        "        base_tr = (1 - self.damping) / num_sents\n",
        "        for _ in range(self.num_iters):\n",
        "            next_text_rank = {i: base_tr for i in range(num_sents)}\n",
        "            for i in range(num_sents):\n",
        "                for j in range(num_sents):\n",
        "                    if i != j and graph[i][j]:\n",
        "                        next_text_rank[j] += self.damping * nw[i][j] * text_rank[i] \n",
        "            delta = sum([abs(text_rank[i] - next_text_rank[i]) for i in range(num_sents)])\n",
        "            if delta <= 0.0000001:\n",
        "                break\n",
        "            text_rank = next_text_rank\n",
        "\n",
        "        ranked = []\n",
        "        for sent_id, w in sorted(list(text_rank.items()), key=lambda x: -x[1]):\n",
        "            ranked.append(sents[sent_id])\n",
        "        return ranked\n",
        "    \n",
        "    def perform(self, text):\n",
        "        sents = [s for s in text.split(\"。\") if len(set(s)) > 1]\n",
        "        return \"。\".join(self.text_rank(sents)[:self.k])"
      ],
      "metadata": {
        "id": "NqkMVb109MJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qDcyTOsnPZl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb6c4961-43dd-4466-b274-144d5168b4ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "（图片来源于网络）文|易不二来源|螳螂财经（ID:TanglangFin）拿了“时尚电商第一股”的名头上市的蘑菇街，才过去大半年，业绩跌落速度却比时尚过气速度还快。截止8月15日收盘，蘑菇街总市值已经跌到只剩2.55亿，与2016年合并美丽说时候的30亿估值相比，已经走到了大失血的地步，与2018年底上市时15亿美元的市值比也相去甚远。成立于2011年的蘑菇街，从最初的女性社区到垂直电商，现在又回到社交电商，几经改变，却一再错失良机。作为电商平台，品类和价格都不具备拼多多的优势；作为内容平台，却又没有小红书的优质内容与影响力。如今，财报上略微好看的直播业务，似乎成了蘑菇街的救命稻草。一度被冠之以电商“第四极”光环的蘑菇街，走到了如今的地步，究竟是怎么了？错失社区“开鱼刀”，蘑菇街进退失据蘑菇街从女性社区转型做垂直电商，是被阿里逼的。最初定位导购平台的蘑菇街和美丽说两家平台，在鼎盛期，一度能每天向淘宝导入300-500万的UV，电商导购转化率高达10%。虽说大树底下好乘凉，但过度依赖淘宝，并不是长久之计。“我要是马云，早把蘑菇街封了。”连蘑菇街联合创始人魏一搏也这样调侃自己。2013年，淘宝取消了蘑菇街和美丽说的淘宝客联盟资格，屏蔽了所有外链，两个平台的商业模式彻底崩塌。改变关头，蘑菇街直接关闭原有的社区，选择了正处于发展高位的垂直电商，此时，不管是唯品会还是聚美优品，都展示着垂直电商无限的生命力。蘑菇街似乎看到了自己无限的未来。然而，垂直电商的生命力几年之后变迅速萎谢，而被蘑菇街关闭的社区，却成为了它错失的“开鱼刀”。“开鱼刀”的典故来自荷兰。17世纪之前，荷兰只是欧洲一个非常普通的国家。森林不广袤，矿产不丰富，倒是鲱鱼数量多到让人震惊。这种体型很小的鱼处理麻烦，不易保存，两三天就臭了。虽然数量多，但荷兰人却无法将其利用起来致富。后来，有一位叫叫威廉·布克尔松的渔民发明了一把能将鲱鱼一刀就轻松处理好的开鱼刀，正是这把小小的开鱼刀，极大地提升了荷兰人处理鲱鱼的效率，处理好的鲱鱼化为盐腌保存，远销世界各国。荷兰经济也由此开始迅速腾飞，成为一代海上霸主。对于企业来说，开鱼刀，就是让自己立足于市场的差异化业务破局点。而被蘑菇街毫不犹豫关闭的UGC社区，却成了小红书手里的“开鱼刀”。2013年，小红书以分享海外旅行购物消费体验等进入年轻人的世界。截至2019年1月，小红书用户数已超过2亿，其中70%用户是90后。如今，社区已经成为小红书的壁垒，是其他平台无法复制的地方。眼看着自己曾经的模式被小红书实践成功，蘑菇街又急又眼红，也打算花大力气重建社区。2019年2月27日，蘑菇街发内部信称，要着力于建立达人社区，集中精力提升社区内容质量，同时，持续完善品牌库。这是要复制小红书？蘑菇街是不是只看到了小红书的成功，却没看到成功背后的焦虑。还是说，蘑菇街已经不知道自己要走什么路了，不如就走一条老路？但是，要知道，小红书自己至今也没有找到一条社区与电商完美结合的变现路径，并且还砍掉了自有品牌“有光”业务，降低电商业务的占比，将重心依然放在社区上。如今，由于内容的失控，小红书还处于下架整改阶段。况且，在“螳螂财经”看来，蘑菇街的社区还有多方面的不足。首先，品类受限，天花板低。蘑菇街定位于为年轻女性消费者提供服饰、鞋包等的穿戴搭配，单一的品类很难成为构建内容护城河。其次，价格低廉，品控不稳。蘑菇街的产品客单价大多在100元左右，价格的美丽，带来的是质量的不美丽，让多数用户种草不拔草。而随着新中产的崛起，品质成为了更关注的问题。CBNData发布的《新时代“自我关爱型”年轻女性金钱观与消费趋势洞察报告》显示，购买贵价和奢价产品的女性消费者中，90后比重逐年上升。还在追求低价的蘑菇街，后期会遇到很大的用户留存难点。最后，用户参与度低。蘑菇街社区的多数帖子都来自于精选达人，用户的内容很少，参与感低，这势必会导致用户的粘性不高。KOL的最大价值在于以自己身影响力和分享的内容吸引到粉丝留存，继而一起参与到内容分享，增加用户的活跃性。目前来说，也许蘑菇街还在以达人内容吸引用户留存吧。如今，UGC社区已然成为各大电商平台的必争之地。此前封杀蘑菇街的阿里，于2018年6月，以3亿美元领投了小红书的D轮融资，让小红书的估值达30亿美元大关。错失先机的蘑菇街，如今在UGC社区上进退失据。进，超越不了小红书；退，已无路可走。起了个大早，还一度手握“开鱼刀”，却赶了个集，实在令人唏嘘。直播这把新的“开鱼刀”，蘑菇街能用好吗？错失社区这把“开鱼刀”后，蘑菇街找到了新的“开鱼刀”。2015年直播火了。2016年3月蘑菇街迅速跟进，正式上线直播功能，开放直播间直接下单功能。之后，APP上留出重要资源位给直播入口，微信上线“蘑菇街购物台”小程序，致力于构建“社交+直播+电商”的新模式。目前来看，蘑菇街似乎用好了这把“开鱼刀”，带来了还不错的成绩。根据蘑菇街招股书，2017财年，蘑菇街直播GMV仅有2亿元，相当于总交易额大约1.4%；2018财年直播贡献GMV为17亿元，占总交易额11.8%；与2017财年相比，无论是绝对值，还是占比，都有较大提升。截至2018年9月30日止的最近6个月内，视频直播GMV继续上扬至14亿元，占总交易额提高到18%。“我们通过丰富内容，提升用户对于直播业务的参与度，促成复购率的提升，继续扩展、优化和提升了时尚生态系统的供应链。”蘑菇街CEO陈琪表示。蘑菇街最新的财报肯定了这一点，本季度蘑菇街的直播业务GMV同比增长177.8%，2018年直播的移动月活用户数增长43.6%，活跃用户平均每天在蘑菇街平台上花费的时间达到54分钟以上。然而，表现亮眼的直播业务背后，却是流量见顶的危机。2018年第四季度，蘑菇街的活跃买家数量为3450万，比去年同期的3390万仅增长1.8%。与此同时，阿里巴巴最新财报显示，淘宝移动月度活跃用户达到6.99亿，较2018年9月增加3300万，巨大的流量给淘宝直播带来的月活用户同比增长超过100%。对比之下，在蘑菇街用户增量很小存量有限的情况下，可以预见，直播业务将会很快摸到天花板。7月底，蘑菇街发布“2019蘑菇街直播双百计划”，计划面向全网招募优质红人主播、机构以及供应链，并将在2019年度内孵化100个销售额破千万的主播。意在以直播为切入点，深入供应链、链接品牌商，形成到用户的闭环，构建自己的时尚生态。蘑菇街死磕直播业务，电商大佬们也没闲着。同样是7月底，京东计划至少投入10亿资源，孵化不超过5名超级网红成为“京品推荐官”；8月1日，网易考拉宣布正式上线直播，初期将以美妆、母婴等平台核心品类为主，通过美妆达人和辣妈KOL直播进行知识类导购；8月3日，微博超级红人节上，微博高级副总裁曹增辉宣布将推出电商服务平台，同时微博电商直播将与淘宝实现打通；8月6日，苏宁易购宣布正式打通快手小店，消费者可通过快手小店跳转至苏宁易购完成购物。淘宝直播有多品类带来的丰富内容；网易考拉有直击海外原产地和产业带的特色IP；苏宁背靠快手的巨大流量......而围绕“人货场”展开直播的蘑菇街，人方面，没有比得过“淘宝一姐”薇娅的高质量主播；货方面，定位服饰和美妆，品类有天然的基因缺陷；场方面，怎样为用户增加更多的场景设计，让直播的内容更丰富好玩，也是之后该解决的难题。各大巨头的进击下，蘑菇街“亮眼”的直播，也成了夹缝求生的业务。但“螳螂财经”认为，蘑菇街也还是有机会的，具体表现在两个方面。第一个，微信赋能蘑菇街小程序。“微信的覆盖人群是最广的，这一渠道不仅拥有支付这一核心基础设施，同时又具备社交功能，再加上小程序这个运营环境，已经开始形成一个生态。”蘑菇街的小程序在上线半年时间获取了9000万客户，直播购物台去年的双十一销量增长则同期上涨将近30倍。第二个，打造时尚生态。“总的来说，我们的目标是形成一横一纵的逻辑。一横即尽可能多地覆盖所有流行的时尚穿搭资讯，一纵则是要打通相关产品在市场中的流通环节，全面满足消费者对时尚的追求与需求，同时扶持更多具有潜力的迷你品牌。”基于时尚这个点，纵横延伸，打造从供应链、KOL、到用户的生态闭环，或将能成为蘑菇街的竞争壁垒。不管是通过微信小程序逐渐把微信用户引流到蘑菇街APP上，还是深入供应链、链接品牌商纵横延伸发展自己的时尚生态，蘑菇街都必须加快步伐。资本的耐心是有限的，而有了阿里扶持的小红书等对手的成长也是飞速的，留给蘑菇街的时间是不多的。未来，随着5G的到来，直播还有很大的想象空间。就看蘑菇街能不能用好这把“开鱼刀”，拯救自己于“扑街”。结语雕爷的文章里曾说“千万别死守某把开鱼刀，一个方法不灵赶紧第二个，第二个不给力咱换第三个，快速切换到能令你战略可以落地的那把最见效率的开鱼刀。”这种切换，就是时刻保持自身跟对手之间的差异化竞争力。对于一直在社区与电商间摇摆的蘑菇街，前期没有找定自己的业务方向，也一再错失撕开差异化竞争的“开鱼刀”，在严重的同质化中，走到市值大失血的地步。希望手握直播“开鱼刀”的蘑菇街，在市场的变化中，拿捏好切换的尺度，别让错失再上演。*此内容为【螳螂财经】原创，未经授权，任何人不得以任何方式使用，包括转载、摘编、复制或建立镜像。【完】螳螂财经（微信ID:TanglangFin）：泛财经新媒体，《财富生活》等多家杂志特约撰稿人。微信十万+曝文《京东走向“四分五裂”》创作者；重点关注：新金融、新零售、上市公司等财经金融等领域。\n",
            "\n",
            "对于一直在社区与电商间摇摆的蘑菇街，前期没有找定自己的业务方向，也一再错失撕开差异化竞争的“开鱼刀”，在严重的同质化中，走到市值大失血的地步。\n",
            "\n",
            "还是说，蘑菇街已经不知道自己要走什么路了，不如就走一条老路？但是，要知道，小红书自己至今也没有找到一条社区与电商完美结合的变现路径，并且还砍掉了自有品牌“有光”业务，降低电商业务的占比，将重心依然放在社区上。淘宝直播有多品类带来的丰富内容；网易考拉有直击海外原产地和产业带的特色IP；苏宁背靠快手的巨大流量......而围绕“人货场”展开直播的蘑菇街，人方面，没有比得过“淘宝一姐”薇娅的高质量主播；货方面，定位服饰和美妆，品类有天然的基因缺陷；场方面，怎样为用户增加更多的场景设计，让直播的内容更丰富好玩，也是之后该解决的难题。蘑菇街最新的财报肯定了这一点，本季度蘑菇街的直播业务GMV同比增长177.8%，2018年直播的移动月活用户数增长43.6%，活跃用户平均每天在蘑菇街平台上花费的时间达到54分钟以上\n",
            "{'rouge-1': {'r': 0.6197183098591549, 'p': 0.12359550561797752, 'f': 0.20608899297423888}, 'rouge-2': {'r': 0.2, 'p': 0.03943661971830986, 'f': 0.06588235294117648}, 'rouge-l': {'r': 0.3380281690140845, 'p': 0.06741573033707865, 'f': 0.11241217798594846}}\n"
          ]
        }
      ],
      "source": [
        "model = TextRankSummarizer()\n",
        "for src, tgt in load_CLTS(\"valid\"):\n",
        "    print(src)\n",
        "    print(tgt)\n",
        "    result = model.perform(src)\n",
        "    print(result)\n",
        "    print(rouge.evaluate([\" \".join(result)], [[\" \".join(tgt)]]))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U ckiptagger\n",
        "from ckiptagger import WS, data_utils\n",
        "data_utils.download_data_gdown(\"./\") #下載模組\n",
        "ws = WS(data_dir='./data', disable_cuda=False)"
      ],
      "metadata": {
        "id": "EtT0S-NbtvQs",
        "outputId": "0072c0b8-8d9c-4a8a-8a4a-91fb1add12e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ckiptagger\n",
            "  Downloading ckiptagger-0.2.1-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: ckiptagger\n",
            "Successfully installed ckiptagger-0.2.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1efHsY16pxK0lBD2gYCgCTnv1Swstq771\n",
            "To: /content/data.zip\n",
            "100%|██████████| 1.88G/1.88G [00:16<00:00, 114MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/ckiptagger/model_ws.py:106: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  cell = tf.compat.v1.nn.rnn_cell.LSTMCell(hidden_d, name=name)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_sentence_list = ws(['我來人工智慧學校上課','sentence_list hello hello'])\n",
        "word_sentence_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpF6kzIfck1e",
        "outputId": "f53f3f9d-5826-4478-99d2-3b790012cfb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['我', '來', '人工', '智慧', '學校', '上課'], ['sentence_list hello hello']]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'google/mt5-small'\n",
        "# !pip uninstall transformers -y\n",
        "!pip install transformers[sentencepiece]\n",
        "# from transformers import T5Tokenizer, TFT5EncoderModel\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "# model = TFT5EncoderModel.from_pretrained(\"t5-small\")\n",
        "# !pip install sentencepiece"
      ],
      "metadata": {
        "id": "yDwOR34VDu4_",
        "outputId": "dd52eb6e-ec87-40cc-c5c7-910a651684c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.7/dist-packages (4.21.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (21.3)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.12.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.8.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (1.21.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (4.12.0)\n",
            "Requirement already satisfied: protobuf<=3.20.1 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (3.17.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.7/dist-packages (from transformers[sentencepiece]) (0.1.97)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers[sentencepiece]) (3.0.9)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1->transformers[sentencepiece]) (1.15.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers[sentencepiece]) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2022.6.15)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers[sentencepiece]) (2.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mdl_name='google/mt5-small'\n",
        "from transformers import AutoModel, AutoTokenizer \n",
        "model_name = \"google/mt5-small\" \n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "input_ids = tokenizer(\n",
        "    \"我來到人工智慧學校上課嚕\"\n",
        ")#.input_ids  # Batch size 1\n",
        "outputs = model(input_ids)\n",
        "outputs"
      ],
      "metadata": {
        "id": "WyTeSSWN_z_C",
        "outputId": "79bebed1-bcb9-4ba5-f856-c41d1a816ac8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 541
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at google/mt5-small were not used when initializing MT5Model: ['lm_head.weight']\n",
            "- This IS expected if you are initializing MT5Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing MT5Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/convert_slow_tokenizer.py:435: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  \"The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option\"\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 247\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'size'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-17cd89f73f91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"我來到人工智慧學校上課嚕\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m )#.input_ids  # Batch size 1\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1401\u001b[0m                 \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1403\u001b[0;31m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1404\u001b[0m             )\n\u001b[1;32m   1405\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseModelOutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/t5/modeling_t5.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    924\u001b[0m             )\n\u001b[1;32m    925\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m             \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    247\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch as t\n",
        "import  torch.nn.functional as F\n",
        "output = F.cosine_similarity(t.tensor([[0,1]]), t.tensor([[0.8]]))\n",
        "output"
      ],
      "metadata": {
        "id": "cZBBNrWO-mMT",
        "outputId": "08d64cd1-a5d3-4c63-b0c8-16c6299e1987",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.7071])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict \n",
        "# tf idf版\n",
        "class TfidfSummarizer:\n",
        "    def __init__(self, k=3, damping=0.85, num_iters=100):\n",
        "        self.damping = damping\n",
        "        self.num_iters = num_iters\n",
        "        self.k = k\n",
        "    \n",
        "    def similarity(self, seq1, seq2):\n",
        "        try:\n",
        "            return F.cosine_similarity(seq1,seq2)\n",
        "        except:\n",
        "            print(seq1)\n",
        "            print(seq1)\n",
        "            # print((log(len(s1)) + log(len(s2))))\n",
        "            return 0\n",
        " \n",
        "    def text_rank(self, sents):\n",
        "        graph = defaultdict(dict) # defaultdict(list) # default值以一個list()方法產生\n",
        "        num_sents = len(sents)\n",
        "        text_rank = {}\n",
        "\n",
        "        for i in range(num_sents):\n",
        "            text_rank[i] = 1.0 / num_sents\n",
        "            for j in range(i + 1, num_sents):\n",
        "                graph[i][j] = self.similarity(sents[i], sents[j])\n",
        "                graph[j][i] = graph[i][j]\n",
        "\n",
        "        nw = defaultdict(dict)\n",
        "        for i in range(num_sents): # 做normalize 動作吧?\n",
        "            s = 0\n",
        "            for j in range(num_sents): #sum 所有 simi\n",
        "                if i != j:\n",
        "                    s += graph[i][j]\n",
        "            for j in range(num_sents): # 除以累加的s\n",
        "                if i != j:\n",
        "                    if s == 0:\n",
        "                        nw[i][j] = 0 \n",
        "                    else:\n",
        "                        nw[i][j] = graph[i][j] / s\n",
        "\n",
        "        base_tr = (1 - self.damping) / num_sents\n",
        "        for _ in range(self.num_iters):\n",
        "            next_text_rank = {i: base_tr for i in range(num_sents)}\n",
        "            for i in range(num_sents):\n",
        "                for j in range(num_sents):\n",
        "                    if i != j and graph[i][j]:\n",
        "                        next_text_rank[j] += self.damping * nw[i][j] * text_rank[i] \n",
        "            delta = sum([abs(text_rank[i] - next_text_rank[i]) for i in range(num_sents)])\n",
        "            if delta <= 0.0000001:\n",
        "                break\n",
        "            text_rank = next_text_rank\n",
        "\n",
        "        ranked = []\n",
        "        for sent_id, w in sorted(list(text_rank.items()), key=lambda x: -x[1]):\n",
        "            ranked.append(sents[sent_id])\n",
        "        return ranked\n",
        "    \n",
        "    def perform(self, text):\n",
        "        sents = [s for s in text.split(\"。\") if len(set(s)) > 1]\n",
        "        return \"。\".join(self.text_rank(sents)[:self.k])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "o2O0fvjOUhvC",
        "outputId": "3a1b410d-896d-4c0a-b2b3-a3456c435dd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'对于一直在社区与电商间摇摆的蘑菇街，前期没有找定自己的业务方向，也一再错失撕开差异化竞争的“开鱼刀”，在严重的同质化中，走到市值大失血的地步。\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation the TextRank Model on the Entire Test Set"
      ],
      "metadata": {
        "id": "-49bhxGUFwNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate(TextRankSummarizer(), \"test\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_AWruQ_FYEL",
        "outputId": "0c55e08c-3980-4bb7-a55a-11feb346cdb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.734778832469057, 'p': 0.19724619622102219, 'f': 0.3110052312048772}, 'rouge-2': {'r': 0.51385642927514, 'p': 0.14433051926616525, 'f': 0.2253620052172279}, 'rouge-l': {'r': 0.5865935999926392, 'p': 0.1617745775377844, 'f': 0.25360760832543405}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate(TextRankSummarizer(), \"test\")) #1千版本"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlbrftGVYeb-",
        "outputId": "4d500695-76b7-49b3-efeb-43a3900da6a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge-1': {'r': 0.7255529855170463, 'p': 0.19565912533375748, 'f': 0.3082049418530782}, 'rouge-2': {'r': 0.4994347756690937, 'p': 0.1409670918443322, 'f': 0.2198740243071592}, 'rouge-l': {'r': 0.5735131732270399, 'p': 0.15884087330009702, 'f': 0.24877949051140333}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(evaluate(TextRankSummarizer(), \"test\")) #1千版本，改tfidf"
      ],
      "metadata": {
        "id": "ze5Z5OCyZQ5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise\n",
        "\n",
        "範例中的 TextRank 是用中文字元為單位計算句子的相似度。請試著改寫上例，使用中文詞彙（word）為單位計算相似度，並實驗何者效果為佳。\n",
        "\n",
        "# Exercise\n",
        "\n",
        "範例中的 TextRank 在計算相似度時，並末排除停用詞（stopwords），請試著改寫上例，並實驗排除停用詞後的效能。\n",
        "\n",
        "# Exercise*\n",
        "\n",
        "範例中的 TextRank 在計算相似度時，並末考慮詞彙（或字元）的重要性。請試著改寫上例，利用 TF-IDF 調配相似度計算，並實驗其效能。\n",
        "\n",
        "# Exercise*\n",
        "\n",
        "範例中的 TextRank 在計算相似度時，不論以字元或詞彙為單位，都是以字符的重疊度來比較相似度。請試著改寫上例，利用 W2V 或 BERT 等詞彙相量來計算相似度，並實驗其效能。\n",
        "\n"
      ],
      "metadata": {
        "id": "DQwI29w8DWXG"
      }
    }
  ]
}