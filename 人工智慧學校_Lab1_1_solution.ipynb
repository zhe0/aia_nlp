{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "人工智慧學校_Lab1-1_solution.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhe0/aia_nlp/blob/main/%E4%BA%BA%E5%B7%A5%E6%99%BA%E6%85%A7%E5%AD%B8%E6%A0%A1_Lab1_1_solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Lab1 Reminder: **\n",
        "1. Make a copy in your google drive to start coding.\n",
        "2. Relink to the demomstration code can check the output."
      ],
      "metadata": {
        "id": "Z-ll7kir2HMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For debugging\n",
        "import pdb\n",
        "\n",
        "# For checking progress\n",
        "from tqdm import tqdm\n",
        "\n",
        "# For loading data\n",
        "import pandas as pd\n",
        "\n",
        "# For tokenizaton\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "# For building n-gram model\n",
        "from collections import Counter, namedtuple\n",
        "import numpy as np\n",
        "\n",
        "# For pos tagging\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TomZNUd72KSl",
        "outputId": "75eee400-feda-4fac-ba44-a50187851924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/APCLab/jieba-tw.git"
      ],
      "metadata": {
        "id": "fw9GceY7btZ9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b91bb99a-63fd-40f4-9cf0-f8f851dd89a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/APCLab/jieba-tw.git\n",
            "  Cloning https://github.com/APCLab/jieba-tw.git to /tmp/pip-req-build-jyqzaqux\n",
            "  Running command git clone -q https://github.com/APCLab/jieba-tw.git /tmp/pip-req-build-jyqzaqux\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1. Data Preprocessing\n",
        "1. show the top-10 common words and their counts before/after preprocessing\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BFsrcQ7Y2No8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Functions and Classes\n",
        "*  Remove the punctuations\n",
        "*  Lower the cases\n",
        "\n"
      ],
      "metadata": {
        "id": "Rw7Tlz_y2WO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_corpus():\n",
        "  \"\"\" Reads and formats the corpus.\n",
        "\n",
        "  Returns:\n",
        "    corpus (list[str]):\n",
        "      A list of sentences in the corpus.\n",
        "  \"\"\"\n",
        "  df = pd.read_csv('https://raw.githubusercontent.com/yilihsu/NLP110/main/data_tiny.csv')\n",
        "  corpus = df.content.to_list()\n",
        "  return corpus"
      ],
      "metadata": {
        "id": "okL0oXDt2LLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(documents):\n",
        "  \"\"\" Preprocesses the corpus.\n",
        "  \n",
        "  Args:\n",
        "    documents (list[str]):\n",
        "      A list of sentences in the corpus.\n",
        "  Returns:\n",
        "    cleaned_documents (list[str]):\n",
        "      A list of cleaned sentences in the corpus.\n",
        "  \"\"\"\n",
        "  cleaned_documents = []\n",
        "  punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~”'''\n",
        "  for doc in documents:\n",
        "    # Tokenizes the sentence\n",
        "    sents = sent_tokenize(doc)\n",
        "\n",
        "    for sent in sents:\n",
        "      # [TODO1]Removes the punctuations, sent = ...\n",
        "      sent = ''.join([char if char not in punc else '' for char in sent])\n",
        "      # [TODO2]Lowers the case, sent = ...\n",
        "      sent = sent.lower()\n",
        "\n",
        "      cleaned_documents.append(sent)\n",
        "\n",
        "  #print(cleaned_documents[:5])\n",
        "  return cleaned_documents"
      ],
      "metadata": {
        "id": "pVGidURt2SLT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute word frequency\n",
        "def get_vocab(documents):\n",
        "  \"\"\" Gets the vocabulary from the corpus.\n",
        "  \n",
        "  Args:\n",
        "    documents (list[str]):\n",
        "      A list of sentences in the corpus\n",
        "  Returns:\n",
        "    vocabulary (collections.Counter)\n",
        "  \"\"\"\n",
        "  vocabulary = Counter()\n",
        "\n",
        "  for doc in tqdm(documents):\n",
        "    tokens = word_tokenize(doc)\n",
        "    vocabulary.update(tokens)\n",
        "\n",
        "  return vocabulary"
      ],
      "metadata": {
        "id": "cM7mx4C52cNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executions\n",
        "### 1. Show the top-10 common words and their counts before/after preprocessing\n"
      ],
      "metadata": {
        "id": "Srmi4Rsr2NO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "raw_documents = get_corpus()\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = get_vocab(raw_documents).most_common(10)\n",
        "print('\\n Before preprocessing:', vocab)\n",
        "\n",
        "# Build vocabulary after preprocessing\n",
        "documents = preprocess(raw_documents)\n",
        "vocab = get_vocab(documents).most_common(10)\n",
        "print('\\n After preprocesing:', vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9593n0iX2erk",
        "outputId": "2496d4af-bdc5-446f-fffd-849b2f27e582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20000/20000 [00:06<00:00, 3043.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Before preprocessing: [('.', 16981), ('the', 9885), (',', 7788), ('to', 7005), ('!', 6642), ('a', 5596), ('is', 5111), ('?', 4640), ('and', 4584), ('you', 4463)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34977/34977 [00:10<00:00, 3313.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " After preprocesing: [('the', 11175), ('to', 7117), ('a', 5847), ('you', 5325), ('is', 5245), ('and', 5087), ('of', 4492), ('i', 3231), ('in', 3203), ('it', 3190)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2. N-Gram Model and POS Tagging\n",
        "1. Build 2-gram / 4-gram model by processed dataset\n",
        "2. Show the top-5 probable next words and their probability after initial token ‘\\<s\\>’ by 2-gram model\n",
        "3. Generate a sentence with 2-gram model and find the POS taggings\n",
        "4. Generate a sentence with 4-gram model and find the POS taggings\n",
        "\n",
        "\n",
        "## Functions and Classes"
      ],
      "metadata": {
        "id": "gasnLFgz2mcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ngram_model(object):\n",
        "  \"\"\" Ngram model implementation.\n",
        "\n",
        "  Attributes:\n",
        "    n (int):\n",
        "      The number of grams to be considered.\n",
        "    model (dict):\n",
        "      The ngram model.\n",
        "  \"\"\"\n",
        "  def __init__(self, documents, N=2):\n",
        "    self.n = N\n",
        "    self.model = self.get_ngram_model(documents)\n",
        "\n",
        "  def get_ngram_model(self, documents):\n",
        "    N = self.n\n",
        "    ngram_model = dict()\n",
        "    full_grams = list()\n",
        "    grams = list()\n",
        "    Word = namedtuple('Word', ['word', 'prob'])\n",
        "\n",
        "    for doc in tqdm(documents):\n",
        "      \n",
        "      # Tokenizes to words\n",
        "      split_words = word_tokenize(doc)\n",
        "\n",
        "      # [TODO3] Append (N-1) start tokens '<s>' and an end token '<\\s>'\n",
        "      \n",
        "      split_words = ['<s>']*(N-1) + split_words + ['<\\s>']\n",
        "\n",
        "      ## [TODO4] Calculates numerator (n-grams)\n",
        "      # for ... in the range of all n-grams \n",
        "      for i in range(len(split_words) - N + 1):\n",
        "        # full_grams.append(tuple(...)), append the tuple of n-grams into full_grams(list)\n",
        "        full_grams.append(tuple(split_words[i:i+N]))\n",
        "\n",
        "      # [TODO5]Calculate denominator ((n-1)-grams)\n",
        "      # for ... in the range of all (n-1)-grams \n",
        "      for i in range(len(split_words) - N + 2):\n",
        "        # grams.append(tuple(...)), append the tuple of (n-1)-grams into grams(list)\n",
        "        grams.append(tuple(split_words[i:i+N-1]))\n",
        "\n",
        "    # Count the occurence frequency of each gram\n",
        "    # Take 2-gram model as example:\n",
        "    #   full_grams -> list[('a', 'gram'),('other', 'gram'), ...]\n",
        "    #   grams -> list[('a',), ('other',), ('gram',), ...]\n",
        "    #   full_gram_counter -> dict{('a', 'gram'):frequency_1, ('other','gram'):frequency_2, ...}\n",
        "    #   gram_counter -> dict{('a'):frequency_3, ('gram'):frequency_4, ...}\n",
        "    full_gram_counter = Counter(full_grams)\n",
        "    # [TODO6] count the grams with Counter\n",
        "    #gram_counter = ...\n",
        "    gram_counter = Counter(grams)\n",
        "    print(grams)\n",
        "\n",
        "    # Build model\n",
        "    # Take 2-gram model as example:\n",
        "    #   { '<s>': [tuple(word='i', prob=0.6), tuple(word='the', prob=0.2), ...],\n",
        "    #   'i': [tuple(word='am', prob=0.7), tuple(word='want', prob=0.1), ...],\n",
        "    #    ... }\n",
        "    for key in full_gram_counter:\n",
        "      word = ''.join(key[:N-1])\n",
        "      #print(word, key[:N-1])\n",
        "      if word not in ngram_model:\n",
        "        ngram_model.update({word: set()})\n",
        "\n",
        "      # next_word_prob -> float\n",
        "      next_word_prob = full_gram_counter[key] / gram_counter[key[:N-1]]\n",
        "      w = Word(key[-1], next_word_prob)\n",
        "      ngram_model[word].add(w)\n",
        "\n",
        "    # Sort the result by frequency\n",
        "    for word, ng in ngram_model.items():\n",
        "      ngram_model[word] = sorted(ng, key=lambda x: x.prob, reverse=True)\n",
        "\n",
        "    return ngram_model\n",
        "\n",
        "\n",
        "  def predict_sent(self, text=None, max_len=30):\n",
        "    \"\"\" Predicts a sentence with the ngram model.\n",
        "\n",
        "    Args:\n",
        "      text (string or list[string])\n",
        "    Returns:\n",
        "      A prediction string.\n",
        "    \"\"\"\n",
        "\n",
        "    N = self.n\n",
        "    backup_tokens = ['<s>']*(N-1)\n",
        "    if not text:\n",
        "      tokens = backup_tokens\n",
        "      output = []\n",
        "\n",
        "    elif type(text)==str:\n",
        "      tokens = backup_tokens + text.split(' ')\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return \n",
        "      output = tokens\n",
        "\n",
        "    elif type(text) == list:\n",
        "      tokens = backup_tokens + text\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return\n",
        "      output = tokens\n",
        "\n",
        "    else:\n",
        "      print('[Error] the input text must be string or list of string')\n",
        "      return\n",
        "\n",
        "    for i in range(max_len):\n",
        "      possible_words = list(self.model[''.join(tokens)])\n",
        "      probs = [word.prob for word in possible_words]\n",
        "      words = [word.word for word in possible_words]\n",
        "      next_word = np.random.choice(words, 1, p=probs)[0]\n",
        "      tokens = tokens[1:] + [next_word]\n",
        "\n",
        "      if next_word == '<\\\\s>':\n",
        "        break\n",
        "\n",
        "      output.append(next_word)\n",
        "    return ' '.join(output)\n",
        "\n",
        "  def predict_next(self, text=None, top=5):\n",
        "    \"\"\" Predicts next word with the ngram model.\n",
        "\n",
        "    Args:\n",
        "      text (string or list[string])\n",
        "\n",
        "    Returns:\n",
        "      possible_next_words (list[namedtuple]):\n",
        "        A list of top few possible next words.\n",
        "    \"\"\"\n",
        "\n",
        "    N = self.n\n",
        "    backup_tokens = ['<s>']*(N-1)\n",
        "    if not text:\n",
        "      tokens = backup_tokens\n",
        "\n",
        "    elif type(text)==str:\n",
        "      tokens = backup_tokens + text.split(' ')\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return \n",
        "\n",
        "    elif type(text) == list:\n",
        "      tokens = backup_tokens + text\n",
        "      tokens = tokens[-(N-1):]\n",
        "      if not self.check_existence(tokens):\n",
        "        return\n",
        "    else:\n",
        "      print('[Error] the input text must be string or list of string')\n",
        "\n",
        "    possible_next_words = self.model[''.join(tokens)][:top]\n",
        "    possible_next_words = [(word.word, word.prob) for word in possible_next_words]\n",
        "\n",
        "    return possible_next_words\n",
        "\n",
        "  def check_existence(self, tokens):\n",
        "    if not ''.join(tokens) in self.model.keys():\n",
        "      print('[Error] the input text {} not in the vocabulary'.format(tokens))\n",
        "      return False\n",
        "    else:\n",
        "      return True"
      ],
      "metadata": {
        "id": "NrPsWg482ikY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Executions\n",
        "### 1. Build 2-gram/4-gram model by processed dataset"
      ],
      "metadata": {
        "id": "mTyP0uT32uQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twogram = Ngram_model(documents, N=2)\n",
        "fourgram = Ngram_model(documents, N=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWQoj6pA2szy",
        "outputId": "626d59ad-4795-467d-d7cf-ac0425f3612d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 34977/34977 [00:08<00:00, 4108.92it/s]\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n",
            "100%|██████████| 34977/34977 [00:04<00:00, 7924.09it/s]\n",
            "IOPub data rate exceeded.\n",
            "The notebook server will temporarily stop sending output\n",
            "to the client in order to avoid crashing it.\n",
            "To change this limit, set the config variable\n",
            "`--NotebookApp.iopub_data_rate_limit`.\n",
            "\n",
            "Current values:\n",
            "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
            "NotebookApp.rate_limit_window=3.0 (secs)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Show the top-5 probable next words and their probability after initial token \\'\\<s\\>\\'  by 2-gram model"
      ],
      "metadata": {
        "id": "PTRU4Gu121NL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = twogram.predict_next(text='<s>', top=5)\n",
        "print('Next word predictions of two gram model:', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfVMnUAo23Bs",
        "outputId": "9ecfd680-9118-4c10-a0cd-b2e3327c7060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Next word predictions of two gram model: [('i', 0.05280612974240215), ('the', 0.03102038482431312), ('you', 0.030248448980758784), ('<\\\\s>', 0.029190610972925066), ('they', 0.019784429768133344)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Generate a sentence with 2-gram model and find the POS taggings\n",
        "\n"
      ],
      "metadata": {
        "id": "3lUpkWeq27yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = twogram.predict_sent(max_len=30)\n",
        "print('Generation results of two gram model:', output)\n",
        "# [TODO7] Find the POS taggings for the generated sentence\n",
        "nltk.pos_tag(word_tokenize(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1yXIx1Io29jW",
        "outputId": "d1218953-fc8a-4d28-bf8d-61571d02e7c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generation results of two gram model: they sent emails\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('they', 'PRP'), ('sent', 'VBD'), ('emails', 'NNS')]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. POS tagging with Chinese input"
      ],
      "metadata": {
        "id": "VCldt1ZDXiwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba.posseg as pseg\n",
        "sentence_1 = \"我最喜歡自然語言處理\"\n",
        "words_1 = pseg.cut(sentence_1)"
      ],
      "metadata": {
        "id": "DIrAmLinXh83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print out the words and their corresponding PoS tags\n",
        "for word, tag in words_1:\n",
        "  print(word, tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcMUWDrXcfd0",
        "outputId": "583e63bf-80ee-4ef2-d574-f7254c09988b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "我 r\n",
            "最 d\n",
            "喜歡 Vt\n",
            "自然 N\n",
            "語言 N\n",
            "處理 Vt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# [TODO8]try your own sentence!\n",
        "# sentence_2 = ...\n",
        "# words_2 = ...\n",
        "sentence_2 = \"受到熱對流發展影響，各地出現午後雷陣雨，中央氣象局發布大雨特報，另針對「臺南市」發布大雷雨即時訊息，持續時間至15時15分止，氣象局提醒民眾外出留意天氣變化。\"\n",
        "words_2 = pseg.cut(sentence_2)\n",
        "print(\"Part of Speech tagging my chinese input: \", sentence_2)\n",
        "for word, tag in words_2:\n",
        "  print(word, tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSqBtEySclau",
        "outputId": "dd2cee75-3e95-4011-dc2e-61125011906c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Part of Speech tagging my chinese input:  受到熱對流發展影響，各地出現午後雷陣雨，中央氣象局發布大雨特報，另針對「臺南市」發布大雷雨即時訊息，持續時間至15時15分止，氣象局提醒民眾外出留意天氣變化。\n",
            "受到 Vt\n",
            "熱對流 N\n",
            "發展 Nv\n",
            "影響 N\n",
            "， x\n",
            "各地 Vi\n",
            "出現 Vi\n",
            "午後 N\n",
            "雷陣雨 N\n",
            "， x\n",
            "中央 N\n",
            "氣象局 N\n",
            "發布 Vt\n",
            "大雨 N\n",
            "特報 N\n",
            "， x\n",
            "另 C\n",
            "針對 P\n",
            "「 x\n",
            "臺南市 N\n",
            "」 x\n",
            "發布 Vt\n",
            "大 Vi\n",
            "雷雨 N\n",
            "即時 ADV\n",
            "訊息 N\n",
            "， x\n",
            "持續 Vt\n",
            "時間 N\n",
            "至 p\n",
            "15 m\n",
            "時 n\n",
            "15 m\n",
            "分止 v\n",
            "， x\n",
            "氣象局 N\n",
            "提醒 Vt\n",
            "民眾 N\n",
            "外出 Vi\n",
            "留意 Vt\n",
            "天氣 N\n",
            "變化 N\n",
            "。 x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9d8TVP3Vc8Re"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}